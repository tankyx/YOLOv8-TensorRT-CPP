================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2024-09-03T20:45:56.103Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
DXGICapture.h
engine.cpp
engine.h
GDIOverlay.cpp
GDIOverlay.h
MouseController.cpp
MouseController.h
object_detection_image_mt.cpp
shader.hlsl
threadsafe_queue.h
yolov8.cpp
yolov8.h

================================================================
Repository Files
================================================================

================
File: DXGICapture.h
================
#include <d3d11.h>
#include <dxgi1_2.h>
#include <opencv2/opencv.hpp>
#include <stdexcept>
#include <vector>
#include <wrl/client.h>
#include <windows.h>
#include <dwmapi.h>

using Microsoft::WRL::ComPtr;

class DXGICapture {
public:
    DXGICapture(HWND hwnd) : targetWnd(hwnd) { Initialize(); }

    ~DXGICapture() { Cleanup(); }

    void CaptureScreen(cv::Mat &frame) {
        ComPtr<IDXGIResource> desktopResource;
        DXGI_OUTDUPL_FRAME_INFO frameInfo;
        HRESULT hr = deskDupl->AcquireNextFrame(0, &frameInfo, &desktopResource);
        if (FAILED(hr)) {
            if (hr == DXGI_ERROR_WAIT_TIMEOUT) {
                return; // No new frame available
            }
            throw std::runtime_error("Failed to acquire next frame.");
        }

        ComPtr<ID3D11Texture2D> acquiredDesktopImage;
        hr = desktopResource.As(&acquiredDesktopImage);
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to query for IDXGISurface interface.");
        }

        // Get the window client area dimensions
        RECT rect = GetWindowRect(targetWnd);
        int width = rect.right - rect.left;
        int height = rect.bottom - rect.top;

        // Copy the specified window region to the CPU accessible texture
        D3D11_BOX box;
        box.left = rect.left;
        box.top = rect.top;
        box.front = 0;
        box.right = rect.right;
        box.bottom = rect.bottom;
        box.back = 1;
        d3dDeviceContext->CopySubresourceRegion(cpuImage.Get(), 0, 0, 0, 0, acquiredDesktopImage.Get(), 0, &box);

        D3D11_MAPPED_SUBRESOURCE mapped;
        hr = d3dDeviceContext->Map(cpuImage.Get(), 0, D3D11_MAP_READ, 0, &mapped);
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to map the copied frame.");
        }

        cv::Mat bgraFrame(height, width, CV_8UC4, mapped.pData, mapped.RowPitch);
        cv::cvtColor(bgraFrame, frame, cv::COLOR_BGRA2BGR);
        d3dDeviceContext->Unmap(cpuImage.Get(), 0);

        hr = deskDupl->ReleaseFrame();
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to release the frame.");
        }
    }

private:
    void Initialize() {
        HRESULT hr = D3D11CreateDevice(nullptr, D3D_DRIVER_TYPE_HARDWARE, nullptr, D3D11_CREATE_DEVICE_BGRA_SUPPORT, nullptr, 0,
                                       D3D11_SDK_VERSION, &d3dDevice, nullptr, &d3dDeviceContext);
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to create D3D11 device.");
        }

        ComPtr<IDXGIDevice> dxgiDevice;
        hr = d3dDevice.As(&dxgiDevice);
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to get DXGI device.");
        }

        ComPtr<IDXGIAdapter> dxgiAdapter;
        hr = dxgiDevice->GetAdapter(&dxgiAdapter);
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to get DXGI adapter.");
        }

        ComPtr<IDXGIOutput> dxgiOutput;
        hr = dxgiAdapter->EnumOutputs(0, &dxgiOutput);
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to get DXGI output.");
        }

        ComPtr<IDXGIOutput1> dxgiOutput1;
        hr = dxgiOutput.As(&dxgiOutput1);
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to get DXGI output1.");
        }

        hr = dxgiOutput1->DuplicateOutput(d3dDevice.Get(), &deskDupl);
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to create desktop duplication.");
        }

        RECT rect = GetWindowRect(targetWnd);
        int width = rect.right - rect.left;
        int height = rect.bottom - rect.top;

        D3D11_TEXTURE2D_DESC desc = {};
        desc.Width = width;
        desc.Height = height;
        desc.MipLevels = 1;
        desc.ArraySize = 1;
        desc.Format = DXGI_FORMAT_B8G8R8A8_UNORM;
        desc.SampleDesc.Count = 1;
        desc.Usage = D3D11_USAGE_STAGING;
        desc.CPUAccessFlags = D3D11_CPU_ACCESS_READ;
        desc.BindFlags = 0;
        hr = d3dDevice->CreateTexture2D(&desc, nullptr, &cpuImage);
        if (FAILED(hr)) {
            throw std::runtime_error("Failed to create CPU-accessible texture.");
        }
    }

    void Cleanup() {
        if (deskDupl) {
            deskDupl->ReleaseFrame();
        }
    }

    RECT GetWindowRect(HWND hwnd) {
        RECT rect;
        GetClientRect(hwnd, &rect);
        MapWindowPoints(hwnd, nullptr, (LPPOINT)&rect, 2);
        return rect;
    }

    HWND targetWnd;
    ComPtr<ID3D11Device> d3dDevice;
    ComPtr<ID3D11DeviceContext> d3dDeviceContext;
    ComPtr<IDXGIOutputDuplication> deskDupl;
    ComPtr<ID3D11Texture2D> cpuImage;
};

================
File: engine.cpp
================
#include "engine.h"
#include <algorithm>
#include <filesystem>
#include <fstream>
#include <iostream>
#include <iterator>
#include <random>

using namespace nvinfer1;
using namespace Util;

std::vector<std::string> Util::getFilesInDirectory(const std::string &dirPath) {
    std::vector<std::string> filepaths;
    for (const auto &entry : std::filesystem::directory_iterator(dirPath)) {
        filepaths.emplace_back(entry.path().string());
    }
    return filepaths;
}

void Logger::log(Severity severity, const char *msg) noexcept {
    // Would advise using a proper logging utility such as
    // https://github.com/gabime/spdlog For the sake of this tutorial, will just
    // log to the console.

    // Only log Warnings or more important.
    if (severity <= Severity::kVERBOSE) {
        std::cout << msg << std::endl;
    }
}

Int8EntropyCalibrator2::Int8EntropyCalibrator2(int32_t batchSize, int32_t inputW, int32_t inputH, const std::string &calibDataDirPath,
                                               const std::string &calibTableName, const std::string &inputBlobName,
                                               const std::array<float, 3> &subVals, const std::array<float, 3> &divVals, bool normalize,
                                               bool readCache)
    : m_batchSize(batchSize), m_inputW(inputW), m_inputH(inputH), m_imgIdx(0), m_calibTableName(calibTableName),
      m_inputBlobName(inputBlobName), m_subVals(subVals), m_divVals(divVals), m_normalize(normalize), m_readCache(readCache) {

    // Allocate GPU memory to hold the entire batch
    m_inputCount = 3 * inputW * inputH * batchSize;
    checkCudaErrorCode(cudaMalloc(&m_deviceInput, m_inputCount * sizeof(float)));

    // Read the name of all the files in the specified directory.
    if (!doesFileExist(calibDataDirPath)) {
        throw std::runtime_error("Error, directory at provided path does not exist: " + calibDataDirPath);
    }

    m_imgPaths = getFilesInDirectory(calibDataDirPath);
    if (m_imgPaths.size() < static_cast<size_t>(batchSize)) {
        throw std::runtime_error("There are fewer calibration images than the specified batch size!");
    }

    // Randomize the calibration data
    auto rd = std::random_device{};
    auto rng = std::default_random_engine{rd()};
    std::shuffle(std::begin(m_imgPaths), std::end(m_imgPaths), rng);
}

int32_t Int8EntropyCalibrator2::getBatchSize() const noexcept {
    // Return the batch size
    return m_batchSize;
}

bool Int8EntropyCalibrator2::getBatch(void **bindings, const char **names, int32_t nbBindings) noexcept {
    // This method will read a batch of images into GPU memory, and place the
    // pointer to the GPU memory in the bindings variable.

    if (m_imgIdx + m_batchSize > static_cast<int>(m_imgPaths.size())) {
        // There are not enough images left to satisfy an entire batch
        return false;
    }

    // Read the calibration images into memory for the current batch
    std::vector<cv::cuda::GpuMat> inputImgs;
    for (int i = m_imgIdx; i < m_imgIdx + m_batchSize; i++) {
        std::cout << "Reading image " << i << ": " << m_imgPaths[i] << std::endl;
        auto cpuImg = cv::imread(m_imgPaths[i]);
        if (cpuImg.empty()) {
            std::cout << "Fatal error: Unable to read image at path: " << m_imgPaths[i] << std::endl;
            return false;
        }

        cv::cuda::GpuMat gpuImg;
        gpuImg.upload(cpuImg);
        cv::cuda::cvtColor(gpuImg, gpuImg, cv::COLOR_BGR2RGB);

        // TODO: Define any preprocessing code here, such as resizing
        auto resized = Engine<float>::resizeKeepAspectRatioPadRightBottom(gpuImg, m_inputH, m_inputW);

        inputImgs.emplace_back(std::move(resized));
    }

    // Convert the batch from NHWC to NCHW
    // ALso apply normalization, scaling, and mean subtraction
    auto mfloat = Engine<float>::blobFromGpuMats(inputImgs, m_subVals, m_divVals, m_normalize);
    auto *dataPointer = mfloat.ptr<void>();

    // Copy the GPU buffer to member variable so that it persists
    checkCudaErrorCode(cudaMemcpyAsync(m_deviceInput, dataPointer, m_inputCount * sizeof(float), cudaMemcpyDeviceToDevice));

    m_imgIdx += m_batchSize;
    if (std::string(names[0]) != m_inputBlobName) {
        std::cout << "Error: Incorrect input name provided!" << std::endl;
        return false;
    }
    bindings[0] = m_deviceInput;
    return true;
}

void const *Int8EntropyCalibrator2::readCalibrationCache(size_t &length) noexcept {
    std::cout << "Searching for calibration cache: " << m_calibTableName << std::endl;
    m_calibCache.clear();
    std::ifstream input(m_calibTableName, std::ios::binary);
    input >> std::noskipws;
    if (m_readCache && input.good()) {
        std::cout << "Reading calibration cache: " << m_calibTableName << std::endl;
        std::copy(std::istream_iterator<char>(input), std::istream_iterator<char>(), std::back_inserter(m_calibCache));
    }
    length = m_calibCache.size();
    return length ? m_calibCache.data() : nullptr;
}

void Int8EntropyCalibrator2::writeCalibrationCache(const void *ptr, std::size_t length) noexcept {
    std::cout << "Writing calib cache: " << m_calibTableName << " Size: " << length << " bytes" << std::endl;
    std::ofstream output(m_calibTableName, std::ios::binary);
    output.write(reinterpret_cast<const char *>(ptr), length);
}

Int8EntropyCalibrator2::~Int8EntropyCalibrator2() { checkCudaErrorCode(cudaFree(m_deviceInput)); };

================
File: engine.h
================
#pragma once

#include "NvInfer.h"
#include "NvOnnxParser.h"
#include <chrono>
#include <cuda_fp16.h>
#include <cuda_runtime.h>
#include <fstream>
#include <opencv2/core/cuda.hpp>
#include <opencv2/cudaarithm.hpp>
#include <opencv2/cudaimgproc.hpp>
#include <opencv2/cudawarping.hpp>
#include <opencv2/opencv.hpp>

// Utility methods
namespace Util {
inline bool doesFileExist(const std::string &filepath) {
    std::ifstream f(filepath.c_str());
    return f.good();
}

inline void checkCudaErrorCode(cudaError_t code) {
    if (code != 0) {
        std::string errMsg = "CUDA operation failed with code: " + std::to_string(code) + "(" + cudaGetErrorName(code) +
                             "), with message: " + cudaGetErrorString(code);
        std::cout << errMsg << std::endl;
        throw std::runtime_error(errMsg);
    }
}

std::vector<std::string> getFilesInDirectory(const std::string &dirPath);
} // namespace Util
// Utility Timer
template <typename Clock = std::chrono::high_resolution_clock> class Stopwatch {
    typename Clock::time_point start_point;

public:
    Stopwatch() : start_point(Clock::now()) {}

    // Returns elapsed time
    template <typename Rep = typename Clock::duration::rep, typename Units = typename Clock::duration> Rep elapsedTime() const {
        std::atomic_thread_fence(std::memory_order_relaxed);
        auto counted_time = std::chrono::duration_cast<Units>(Clock::now() - start_point).count();
        std::atomic_thread_fence(std::memory_order_relaxed);
        return static_cast<Rep>(counted_time);
    }
};

using preciseStopwatch = Stopwatch<>;

// Precision used for GPU inference
enum class Precision {
    // Full precision floating point value
    FP32,
    // Half prevision floating point value
    FP16,
    // Int8 quantization.
    // Has reduced dynamic range, may result in slight loss in accuracy.
    // If INT8 is selected, must provide path to calibration dataset directory.
    INT8,
};

// Options for the network
struct Options {
    // Precision to use for GPU inference.
    Precision precision = Precision::FP16;
    // If INT8 precision is selected, must provide path to calibration dataset
    // directory.
    std::string calibrationDataDirectoryPath;
    // The batch size to be used when computing calibration data for INT8
    // inference. Should be set to as large a batch number as your GPU will
    // support.
    int32_t calibrationBatchSize = 128;
    // The batch size which should be optimized for.
    int32_t optBatchSize = 1;
    // Maximum allowable batch size
    int32_t maxBatchSize = 16;
    // GPU device index
    int deviceIndex = 0;
};

// Class used for int8 calibration
class Int8EntropyCalibrator2 : public nvinfer1::IInt8EntropyCalibrator2 {
public:
    Int8EntropyCalibrator2(int32_t batchSize, int32_t inputW, int32_t inputH, const std::string &calibDataDirPath,
                           const std::string &calibTableName, const std::string &inputBlobName,
                           const std::array<float, 3> &subVals = {0.f, 0.f, 0.f}, const std::array<float, 3> &divVals = {1.f, 1.f, 1.f},
                           bool normalize = true, bool readCache = true);
    virtual ~Int8EntropyCalibrator2();
    // Abstract base class methods which must be implemented
    int32_t getBatchSize() const noexcept override;
    bool getBatch(void *bindings[], char const *names[], int32_t nbBindings) noexcept override;
    void const *readCalibrationCache(std::size_t &length) noexcept override;
    void writeCalibrationCache(void const *ptr, std::size_t length) noexcept override;

private:
    const int32_t m_batchSize;
    const int32_t m_inputW;
    const int32_t m_inputH;
    int32_t m_imgIdx;
    std::vector<std::string> m_imgPaths;
    size_t m_inputCount;
    const std::string m_calibTableName;
    const std::string m_inputBlobName;
    const std::array<float, 3> m_subVals;
    const std::array<float, 3> m_divVals;
    const bool m_normalize;
    const bool m_readCache;
    void *m_deviceInput;
    std::vector<char> m_calibCache;
};

// Class to extend TensorRT logger
class Logger : public nvinfer1::ILogger {
    void log(Severity severity, const char *msg) noexcept override;
};

template <typename T> class Engine {
public:
    Engine(const Options &options);
    ~Engine();

    // Build the onnx model into a TensorRT engine file, cache the model to disk
    // (to avoid rebuilding in future), and then load the model into memory The
    // default implementation will normalize values between [0.f, 1.f] Setting the
    // normalize flag to false will leave values between [0.f, 255.f] (some
    // converted models may require this). If the model requires values to be
    // normalized between [-1.f, 1.f], use the following params:
    //    subVals = {0.5f, 0.5f, 0.5f};
    //    divVals = {0.5f, 0.5f, 0.5f};
    //    normalize = true;
    bool buildLoadNetwork(std::string onnxModelPath, const std::array<float, 3> &subVals = {0.f, 0.f, 0.f},
                          const std::array<float, 3> &divVals = {1.f, 1.f, 1.f}, bool normalize = true);

    // Load a TensorRT engine file from disk into memory
    // The default implementation will normalize values between [0.f, 1.f]
    // Setting the normalize flag to false will leave values between [0.f, 255.f]
    // (some converted models may require this). If the model requires values to
    // be normalized between [-1.f, 1.f], use the following params:
    //    subVals = {0.5f, 0.5f, 0.5f};
    //    divVals = {0.5f, 0.5f, 0.5f};
    //    normalize = true;
    bool loadNetwork(std::string trtModelPath, const std::array<float, 3> &subVals = {0.f, 0.f, 0.f},
                     const std::array<float, 3> &divVals = {1.f, 1.f, 1.f}, bool normalize = true);

    // Run inference.
    // Input format [input][batch][cv::cuda::GpuMat]
    // Output format [batch][output][feature_vector]
    bool runInference(const std::vector<std::vector<cv::cuda::GpuMat>> &inputs, std::vector<std::vector<std::vector<T>>> &featureVectors);

    // Utility method for resizing an image while maintaining the aspect ratio by
    // adding padding to smaller dimension after scaling While letterbox padding
    // normally adds padding to top & bottom, or left & right sides, this
    // implementation only adds padding to the right or bottom side This is done
    // so that it's easier to convert detected coordinates (ex. YOLO model) back
    // to the original reference frame.
    static cv::cuda::GpuMat resizeKeepAspectRatioPadRightBottom(const cv::cuda::GpuMat &input, size_t height, size_t width,
                                                                const cv::Scalar &bgcolor = cv::Scalar(0, 0, 0));

    [[nodiscard]] const std::vector<nvinfer1::Dims3> &getInputDims() const { return m_inputDims; };
    [[nodiscard]] const std::vector<nvinfer1::Dims> &getOutputDims() const { return m_outputDims; };

    // Utility method for transforming triple nested output array into 2D array
    // Should be used when the output batch size is 1, but there are multiple
    // output feature vectors
    static void transformOutput(std::vector<std::vector<std::vector<T>>> &input, std::vector<std::vector<T>> &output);

    // Utility method for transforming triple nested output array into single
    // array Should be used when the output batch size is 1, and there is only a
    // single output feature vector
    static void transformOutput(std::vector<std::vector<std::vector<T>>> &input, std::vector<T> &output);
    // Convert NHWC to NCHW and apply scaling and mean subtraction
    static cv::cuda::GpuMat blobFromGpuMats(const std::vector<cv::cuda::GpuMat> &batchInput, const std::array<float, 3> &subVals,
                                            const std::array<float, 3> &divVals, bool normalize);

    void setCaptureDimensions(int height, int width) {
		captureHeight = height;
		captureWidth = width;
	}

private:
    // Build the network
    bool build(std::string onnxModelPath, const std::array<float, 3> &subVals, const std::array<float, 3> &divVals, bool normalize);

    // Converts the engine options into a string
    std::string serializeEngineOptions(const Options &options, const std::string &onnxModelPath);

    void getDeviceNames(std::vector<std::string> &deviceNames);

    void clearGpuBuffers();

    // Normalization, scaling, and mean subtraction of inputs
    std::array<float, 3> m_subVals{};
    std::array<float, 3> m_divVals{};
    bool m_normalize;

    // Holds pointers to the input and output GPU buffers
    std::vector<void *> m_buffers;
    std::vector<uint32_t> m_outputLengths{};
    std::vector<nvinfer1::Dims3> m_inputDims;
    std::vector<nvinfer1::Dims> m_outputDims;
    std::vector<std::string> m_IOTensorNames;
    int32_t m_inputBatchSize;

    // Must keep IRuntime around for inference, see:
    // https://forums.developer.nvidia.com/t/is-it-safe-to-deallocate-nvinfer1-iruntime-after-creating-an-nvinfer1-icudaengine-but-before-running-inference-with-said-icudaengine/255381/2?u=cyruspk4w6
    std::unique_ptr<nvinfer1::IRuntime> m_runtime = nullptr;
    std::unique_ptr<Int8EntropyCalibrator2> m_calibrator = nullptr;
    std::unique_ptr<nvinfer1::ICudaEngine> m_engine = nullptr;
    std::unique_ptr<nvinfer1::IExecutionContext> m_context = nullptr;
    const Options m_options;
    Logger m_logger;

    int captureHeight;
    int captureWidth;
};

template <typename T> Engine<T>::Engine(const Options &options) : m_options(options) {}

template <typename T> Engine<T>::~Engine() { clearGpuBuffers(); }

template <typename T> void Engine<T>::clearGpuBuffers() {
    if (!m_buffers.empty()) {
        // Free GPU memory of outputs
        const auto numInputs = m_inputDims.size();
        for (int32_t outputBinding = numInputs; outputBinding < m_engine->getNbIOTensors(); ++outputBinding) {
            Util::checkCudaErrorCode(cudaFree(m_buffers[outputBinding]));
        }
        m_buffers.clear();
    }
}

template <typename T>
bool Engine<T>::buildLoadNetwork(std::string onnxModelPath, const std::array<float, 3> &subVals, const std::array<float, 3> &divVals,
                                 bool normalize) {
    // Only regenerate the engine file if it has not already been generated for
    // the specified options, otherwise load cached version from disk
    const auto engineName = serializeEngineOptions(m_options, onnxModelPath);
    std::cout << "Searching for engine file with name: " << engineName << std::endl;

    if (Util::doesFileExist(engineName)) {
        std::cout << "Engine found, not regenerating..." << std::endl;
    } else {
        if (!Util::doesFileExist(onnxModelPath)) {
            throw std::runtime_error("Could not find onnx model at path: " + onnxModelPath);
        }

        // Was not able to find the engine file, generate...
        std::cout << "Engine not found, generating. This could take a while..." << std::endl;

        // Build the onnx model into a TensorRT engine
        auto ret = build(onnxModelPath, subVals, divVals, normalize);
        if (!ret) {
            return false;
        }
    }

    // Load the TensorRT engine file into memory
    return loadNetwork(engineName, subVals, divVals, normalize);
}

template <typename T>
bool Engine<T>::loadNetwork(std::string trtModelPath, const std::array<float, 3> &subVals, const std::array<float, 3> &divVals,
                            bool normalize) {
    m_subVals = subVals;
    m_divVals = divVals;
    m_normalize = normalize;

    // Read the serialized model from disk
    if (!Util::doesFileExist(trtModelPath)) {
        std::cout << "Error, unable to read TensorRT model at path: " + trtModelPath << std::endl;
        return false;
    } else {
        std::cout << "Loading TensorRT engine file at path: " << trtModelPath << std::endl;
    }

    std::ifstream file(trtModelPath, std::ios::binary | std::ios::ate);
    std::streamsize size = file.tellg();
    file.seekg(0, std::ios::beg);

    std::vector<char> buffer(size);
    if (!file.read(buffer.data(), size)) {
        throw std::runtime_error("Unable to read engine file");
    }

    std::cout << "Successfully read engine file" << std::endl;
    // Create a runtime to deserialize the engine file.
    m_runtime = std::unique_ptr<nvinfer1::IRuntime>{nvinfer1::createInferRuntime(m_logger)};
    if (!m_runtime) {
        return false;
    }
    std::cout << "Successfully created runtime" << std::endl;

    // Set the device index
    auto ret = cudaSetDevice(m_options.deviceIndex);
    if (ret != 0) {
        int numGPUs;
        cudaGetDeviceCount(&numGPUs);
        auto errMsg = "Unable to set GPU device index to: " + std::to_string(m_options.deviceIndex) + ". Note, your device has " +
                      std::to_string(numGPUs) + " CUDA-capable GPU(s).";
        throw std::runtime_error(errMsg);
    }
    std::cout << "Successfully set device index to: " << m_options.deviceIndex << std::endl;

    // Create an engine, a representation of the optimized model.
    m_engine = std::unique_ptr<nvinfer1::ICudaEngine>(m_runtime->deserializeCudaEngine(buffer.data(), buffer.size()));
    if (!m_engine) {
        return false;
    }
    std::cout << "Successfully deserialized engine" << std::endl;

    // The execution context contains all of the state associated with a
    // particular invocation
    m_context = std::unique_ptr<nvinfer1::IExecutionContext>(m_engine->createExecutionContext());
    if (!m_context) {
        return false;
    }
    std::cout << "Successfully created execution context" << std::endl;

    // Storage for holding the input and output buffers
    // This will be passed to TensorRT for inference
    clearGpuBuffers();
    m_buffers.resize(m_engine->getNbIOTensors());
    std::cout << "Successfully allocated memory for input and output buffers" << std::endl;

    m_outputLengths.clear();
    m_inputDims.clear();
    m_outputDims.clear();
    m_IOTensorNames.clear();
    std::cout << "Successfully loaded engine file" << std::endl;

    // Create a cuda stream
    cudaStream_t stream;
    Util::checkCudaErrorCode(cudaStreamCreate(&stream));
    std::cout << "Successfully created CUDA stream" << std::endl;

    // Allocate GPU memory for input and output buffers
    m_outputLengths.clear(); 
    for (int i = 0; i < m_engine->getNbIOTensors(); ++i) {
        const auto tensorName = m_engine->getIOTensorName(i);
        m_IOTensorNames.emplace_back(tensorName);
        const auto tensorType = m_engine->getTensorIOMode(tensorName);
        const auto tensorShape = m_engine->getTensorShape(tensorName);
        const auto tensorDataType = m_engine->getTensorDataType(tensorName);

        if (tensorType == nvinfer1::TensorIOMode::kINPUT) {
            // The implementation currently only supports inputs of type float
            if (m_engine->getTensorDataType(tensorName) != nvinfer1::DataType::kFLOAT &&
                m_engine->getTensorDataType(tensorName) != nvinfer1::DataType::kHALF) {
                throw std::runtime_error("Error, the implementation currently only supports float and half inputs");
            }

            // Don't need to allocate memory for inputs as we will be using the OpenCV
            // GpuMat buffer directly.

            // Store the input dims for later use
            m_inputDims.emplace_back(tensorShape.d[1], tensorShape.d[2], tensorShape.d[3]);
            m_inputBatchSize = tensorShape.d[0];
            std::cout << "Input batch size: " << m_inputBatchSize << std::endl;
        } else if (tensorType == nvinfer1::TensorIOMode::kOUTPUT) {
            // Ensure the model output data type matches the template argument
            // specified by the user
            if (tensorDataType == nvinfer1::DataType::kFLOAT && !std::is_same<float, T>::value) {
                throw std::runtime_error("Error, the model has expected output of type float. Engine class "
                                         "template parameter must be adjusted.");
            } else if (tensorDataType == nvinfer1::DataType::kHALF && !std::is_same<__half, T>::value) {
                throw std::runtime_error("Error, the model has expected output of type __half. Engine class "
                                         "template parameter must be adjusted.");
            } else if (tensorDataType == nvinfer1::DataType::kINT8 && !std::is_same<int8_t, T>::value) {
                throw std::runtime_error("Error, the model has expected output of type int8_t. Engine class "
                                         "template parameter must be adjusted.");
            } else if (tensorDataType == nvinfer1::DataType::kINT32 && !std::is_same<int32_t, T>::value) {
                throw std::runtime_error("Error, the model has expected output of type int32_t. Engine "
                                         "class template parameter must be adjusted.");
            } else if (tensorDataType == nvinfer1::DataType::kBOOL && !std::is_same<bool, T>::value) {
                throw std::runtime_error("Error, the model has expected output of type bool. Engine class "
                                         "template parameter must be adjusted.");
            } else if (tensorDataType == nvinfer1::DataType::kUINT8 && !std::is_same<uint8_t, T>::value) {
                throw std::runtime_error("Error, the model has expected output of type uint8_t. Engine "
                                         "class template parameter must be adjusted.");
            } else if (tensorDataType == nvinfer1::DataType::kFP8) {
                throw std::runtime_error("Error, model has unsupported output type");
            }

            // The binding is an output
            uint32_t outputLength = 1;
            m_outputDims.push_back(tensorShape);

            for (int j = 1; j < tensorShape.nbDims; ++j) {
                // We ignore j = 0 because that is the batch size, and we will take that
                // into account when sizing the buffer
                outputLength *= tensorShape.d[j];
            }

            m_outputLengths.push_back(outputLength);
            // Now size the output buffer appropriately, taking into account the max
            // possible batch size (although we could actually end up using less
            // memory)
            Util::checkCudaErrorCode(cudaMallocAsync(&m_buffers[i], outputLength * m_options.maxBatchSize * sizeof(T), stream));
            std::cout << "Allocated memory for output buffer: " << tensorName << std::endl;
        } else {
            throw std::runtime_error("Error, IO Tensor is neither an input or output!");
        }
    }

    // Synchronize and destroy the cuda stream
    Util::checkCudaErrorCode(cudaStreamSynchronize(stream));
    Util::checkCudaErrorCode(cudaStreamDestroy(stream));

    return true;
}

template <typename T>
bool Engine<T>::build(std::string onnxModelPath, const std::array<float, 3> &subVals, const std::array<float, 3> &divVals, bool normalize) {
    // Create our engine builder.
    auto builder = std::unique_ptr<nvinfer1::IBuilder>(nvinfer1::createInferBuilder(m_logger));
    if (!builder) {
        return false;
    }

    // Define an explicit batch size and then create the network (implicit batch
    // size is deprecated). More info here:
    // https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#explicit-implicit-batch
    auto explicitBatch = 1U << static_cast<uint32_t>(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);
    auto network = std::unique_ptr<nvinfer1::INetworkDefinition>(builder->createNetworkV2(explicitBatch));
    if (!network) {
        return false;
    }

    // Create a parser for reading the onnx file.
    auto parser = std::unique_ptr<nvonnxparser::IParser>(nvonnxparser::createParser(*network, m_logger));
    if (!parser) {
        return false;
    }

    // We are going to first read the onnx file into memory, then pass that buffer
    // to the parser. Had our onnx model file been encrypted, this approach would
    // allow us to first decrypt the buffer.
    std::ifstream file(onnxModelPath, std::ios::binary | std::ios::ate);
    std::streamsize size = file.tellg();
    file.seekg(0, std::ios::beg);

    std::vector<char> buffer(size);
    if (!file.read(buffer.data(), size)) {
        throw std::runtime_error("Unable to read engine file");
    }

    // Parse the buffer we read into memory.
    auto parsed = parser->parse(buffer.data(), buffer.size());
    if (!parsed) {
        return false;
    }

    // Ensure that all the inputs have the same batch size
    const auto numInputs = network->getNbInputs();
    if (numInputs < 1) {
        throw std::runtime_error("Error, model needs at least 1 input!");
    }
    const auto input0Batch = network->getInput(0)->getDimensions().d[0];
    for (int32_t i = 1; i < numInputs; ++i) {
        if (network->getInput(i)->getDimensions().d[0] != input0Batch) {
            throw std::runtime_error("Error, the model has multiple inputs, each "
                                     "with differing batch sizes!");
        }
    }

    // Check to see if the model supports dynamic batch size or not
    bool doesSupportDynamicBatch = false;
    if (input0Batch == -1) {
        doesSupportDynamicBatch = true;
        std::cout << "Model supports dynamic batch size" << std::endl;
    } else {
        std::cout << "Model only supports fixed batch size of " << input0Batch << std::endl;
        // If the model supports a fixed batch size, ensure that the maxBatchSize
        // and optBatchSize were set correctly.
        if (m_options.optBatchSize != input0Batch || m_options.maxBatchSize != input0Batch) {
            throw std::runtime_error("Error, model only supports a fixed batch size of " + std::to_string(input0Batch) +
                                     ". Must set Options.optBatchSize and Options.maxBatchSize to 1");
        }
    }

    auto config = std::unique_ptr<nvinfer1::IBuilderConfig>(builder->createBuilderConfig());
    if (!config) {
        return false;
    }

    // Register a single optimization profile
    nvinfer1::IOptimizationProfile *optProfile = builder->createOptimizationProfile();

    for (int32_t i = 0; i < numInputs; ++i) {
        // Must specify dimensions for all the inputs the model expects.
        const auto input = network->getInput(i);
        const auto inputName = input->getName();
        const auto inputDims = input->getDimensions();
        int32_t inputC = inputDims.d[1];
        int32_t inputH = inputDims.d[2];
        int32_t inputW = inputDims.d[3];

        std::cout << "Input name: " << inputName << ", dimensions: " << inputC << "x" << inputH << "x" << inputW << std::endl;

        // Specify the optimization profile`
        if (doesSupportDynamicBatch) {
            optProfile->setDimensions(inputName, nvinfer1::OptProfileSelector::kMIN, nvinfer1::Dims4(1, inputC, inputH, inputW));
        } else {
            optProfile->setDimensions(inputName, nvinfer1::OptProfileSelector::kMIN,
                                      nvinfer1::Dims4(m_options.optBatchSize, inputC, inputH, inputW));
        }
        optProfile->setDimensions(inputName, nvinfer1::OptProfileSelector::kOPT,
                                  nvinfer1::Dims4(m_options.optBatchSize, inputC, inputH, inputW));
        optProfile->setDimensions(inputName, nvinfer1::OptProfileSelector::kMAX,
                                  nvinfer1::Dims4(m_options.maxBatchSize, inputC, inputH, inputW));
    }
    config->addOptimizationProfile(optProfile);

    // Set the precision level
    const auto engineName = serializeEngineOptions(m_options, onnxModelPath);
    if (m_options.precision == Precision::FP16) {
        // Ensure the GPU supports FP16 inference
        if (!builder->platformHasFastFp16()) {
            throw std::runtime_error("Error: GPU does not support FP16 precision");
        } else {
			std::cout << "Using FP16 precision" << std::endl;
		}
        config->setFlag(nvinfer1::BuilderFlag::kFP16);
    } else if (m_options.precision == Precision::INT8) {
        if (numInputs > 1) {
            throw std::runtime_error("Error, this implementation currently only supports INT8 "
                                     "quantization for single input models");
        }

        // Ensure the GPU supports INT8 Quantization
        if (!builder->platformHasFastInt8()) {
            throw std::runtime_error("Error: GPU does not support INT8 precision");
        }

        // Ensure the user has provided path to calibration data directory
        if (m_options.calibrationDataDirectoryPath.empty()) {
            throw std::runtime_error("Error: If INT8 precision is selected, must provide path to "
                                     "calibration data directory to Engine::build method");
        }
        std::cout << "Using INT8 precision" << std::endl;
        config->setFlag((nvinfer1::BuilderFlag::kINT8));

        const auto input = network->getInput(0);
        const auto inputName = input->getName();
        const auto inputDims = input->getDimensions();
        const auto calibrationFileName = engineName + ".calibration";

        m_calibrator = std::make_unique<Int8EntropyCalibrator2>(m_options.calibrationBatchSize, inputDims.d[3], inputDims.d[2],
                                                                m_options.calibrationDataDirectoryPath, calibrationFileName, inputName,
                                                                subVals, divVals, normalize);
        config->setInt8Calibrator(m_calibrator.get());
        std::cout << "Calibration data will be saved to: " << calibrationFileName << std::endl;
    }

    // CUDA stream used for profiling by the builder.
    cudaStream_t profileStream;
    Util::checkCudaErrorCode(cudaStreamCreate(&profileStream));
    std::cout << "Using CUDA stream for profiling" << std::endl;
    config->setProfileStream(profileStream);

    // Build the engine
    // If this call fails, it is suggested to increase the logger verbosity to
    // kVERBOSE and try rebuilding the engine. Doing so will provide you with more
    // information on why exactly it is failing.
    std::unique_ptr<nvinfer1::IHostMemory> plan{builder->buildSerializedNetwork(*network, *config)};
    if (!plan) {
        return false;
    }
    std::cout << "Successfully built TensorRT engine" << std::endl;
    // Write the engine to disk
    std::ofstream outfile(engineName, std::ofstream::binary);
    outfile.write(reinterpret_cast<const char *>(plan->data()), plan->size());

    std::cout << "Success, saved engine to " << engineName << std::endl;

    Util::checkCudaErrorCode(cudaStreamDestroy(profileStream));
    return true;
}

template <typename T>
bool Engine<T>::runInference(const std::vector<std::vector<cv::cuda::GpuMat>> &inputs,
                             std::vector<std::vector<std::vector<T>>> &featureVectors) {
    // First we do some error checking
    if (inputs.empty() || inputs[0].empty()) {
        std::cout << "===== Error =====" << std::endl;
        std::cout << "Provided input vector is empty!" << std::endl;
        return false;
    }

    const auto numInputs = m_inputDims.size();
    if (inputs.size() != numInputs) {
        std::cout << "===== Error =====" << std::endl;
        std::cout << "Incorrect number of inputs provided!" << std::endl;
        return false;
    }

    // Ensure the batch size does not exceed the max
    if (inputs[0].size() > static_cast<size_t>(m_options.maxBatchSize)) {
        std::cout << "===== Error =====" << std::endl;
        std::cout << "The batch size is larger than the model expects!" << std::endl;
        std::cout << "Model max batch size: " << m_options.maxBatchSize << std::endl;
        std::cout << "Batch size provided to call to runInference: " << inputs[0].size() << std::endl;
        return false;
    }

    // Ensure that if the model has a fixed batch size that is greater than 1, the
    // input has the correct length
    if (m_inputBatchSize != -1 && inputs[0].size() != static_cast<size_t>(m_inputBatchSize)) {
        std::cout << "===== Error =====" << std::endl;
        std::cout << "The batch size is different from what the model expects!" << std::endl;
        std::cout << "Model batch size: " << m_inputBatchSize << std::endl;
        std::cout << "Batch size provided to call to runInference: " << inputs[0].size() << std::endl;
        return false;
    }

    const auto batchSize = static_cast<int32_t>(inputs[0].size());
    // Make sure the same batch size was provided for all inputs
    for (size_t i = 1; i < inputs.size(); ++i) {
        if (inputs[i].size() != static_cast<size_t>(batchSize)) {
            std::cout << "===== Error =====" << std::endl;
            std::cout << "The batch size needs to be constant for all inputs!" << std::endl;
            return false;
        }
    }

    // Create the cuda stream that will be used for inference
    cudaStream_t inferenceCudaStream;
    Util::checkCudaErrorCode(cudaStreamCreate(&inferenceCudaStream));

    std::vector<cv::cuda::GpuMat> preprocessedInputs;

    for (size_t i = 0; i < numInputs; ++i) {
        const auto &batchInput = inputs[i];
        const auto &dims = m_inputDims[i];

        auto &input = batchInput[0];
        if (input.channels() != dims.d[0] || input.rows != dims.d[1] || input.cols != dims.d[2]) {
            std::cout << "===== Error =====" << std::endl;
            std::cout << "Input does not have correct size!" << std::endl;
            std::cout << "Expected: (" << dims.d[0] << ", " << dims.d[1] << ", " << dims.d[2] << ")" << std::endl;
            std::cout << "Got: (" << input.channels() << ", " << input.rows << ", " << input.cols << ")" << std::endl;
            std::cout << "Ensure you resize your input image to the correct size" << std::endl;
            return false;
        }

        nvinfer1::Dims4 inputDims = {batchSize, dims.d[0], dims.d[1], dims.d[2]};
        m_context->setInputShape(m_IOTensorNames[i].c_str(),
                                 inputDims); // Define the batch size

        // OpenCV reads images into memory in NHWC format, while TensorRT expects
        // images in NCHW format. The following method converts NHWC to NCHW. Even
        // though TensorRT expects NCHW at IO, during optimization, it can
        // internally use NHWC to optimize cuda kernels See:
        // https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#data-layout
        // Copy over the input data and perform the preprocessing
        auto mfloat = blobFromGpuMats(batchInput, m_subVals, m_divVals, m_normalize);
        preprocessedInputs.push_back(mfloat);
        m_buffers[i] = mfloat.ptr<void>();
    }

    // Ensure all dynamic bindings have been defined.
    if (!m_context->allInputDimensionsSpecified()) {
        throw std::runtime_error("Error, not all required dimensions specified.");
    }

    // Set the address of the input and output buffers
    for (size_t i = 0; i < m_buffers.size(); ++i) {
        bool status = m_context->setTensorAddress(m_IOTensorNames[i].c_str(), m_buffers[i]);
        if (!status) {
            return false;
        }
    }

    // Run inference.
    bool status = m_context->enqueueV3(inferenceCudaStream);
    if (!status) {
        return false;
    }

    // Copy the outputs back to CPU
    featureVectors.clear();

    for (int batch = 0; batch < batchSize; ++batch) {
        // Batch
        std::vector<std::vector<T>> batchOutputs{};
        for (int32_t outputBinding = numInputs; outputBinding < m_engine->getNbIOTensors(); ++outputBinding) {
            // We start at index m_inputDims.size() to account for the inputs in our
            // m_buffers
            std::vector<T> output;
            auto outputLength = m_outputLengths[outputBinding - numInputs];
            output.resize(outputLength);
            // Copy the output
            Util::checkCudaErrorCode(cudaMemcpyAsync(output.data(),
                                                     static_cast<char *>(m_buffers[outputBinding]) + (batch * sizeof(T) * outputLength),
                                                     outputLength * sizeof(T), cudaMemcpyDeviceToHost, inferenceCudaStream));
            batchOutputs.emplace_back(std::move(output));
        }
        featureVectors.emplace_back(std::move(batchOutputs));
    }

    Util::checkCudaErrorCode(cudaStreamSynchronize(inferenceCudaStream));
    Util::checkCudaErrorCode(cudaStreamDestroy(inferenceCudaStream));

    return true;
}

template <typename T>
cv::cuda::GpuMat Engine<T>::blobFromGpuMats(const std::vector<cv::cuda::GpuMat> &batchInput, const std::array<float, 3> &subVals,
                                            const std::array<float, 3> &divVals, bool normalize) {
    cv::cuda::GpuMat gpu_dst(1, batchInput[0].rows * batchInput[0].cols * batchInput.size(), CV_8UC3);

    size_t width = batchInput[0].cols * batchInput[0].rows;
    for (size_t img = 0; img < batchInput.size(); img++) {
        std::vector<cv::cuda::GpuMat> input_channels{
            cv::cuda::GpuMat(batchInput[0].rows, batchInput[0].cols, CV_8U, &(gpu_dst.ptr()[0 + width * 3 * img])),
            cv::cuda::GpuMat(batchInput[0].rows, batchInput[0].cols, CV_8U, &(gpu_dst.ptr()[width + width * 3 * img])),
            cv::cuda::GpuMat(batchInput[0].rows, batchInput[0].cols, CV_8U, &(gpu_dst.ptr()[width * 2 + width * 3 * img]))};
        cv::cuda::split(batchInput[img], input_channels); // HWC -> CHW
    }

    cv::cuda::GpuMat mfloat;
    if (normalize) {
        // [0.f, 1.f]
        gpu_dst.convertTo(mfloat, CV_32FC3, 1.f / 255.f);
    } else {
        // [0.f, 255.f]
        gpu_dst.convertTo(mfloat, CV_32FC3);
    }

    // Apply scaling and mean subtraction
    cv::cuda::subtract(mfloat, cv::Scalar(subVals[0], subVals[1], subVals[2]), mfloat, cv::noArray(), -1);
    cv::cuda::divide(mfloat, cv::Scalar(divVals[0], divVals[1], divVals[2]), mfloat, 1, -1);

    return mfloat;
}

template <typename T> std::string Engine<T>::serializeEngineOptions(const Options &options, const std::string &onnxModelPath) {
    const auto filenamePos = onnxModelPath.find_last_of('/') + 1;
    std::string engineName = onnxModelPath.substr(filenamePos, onnxModelPath.find_last_of('.') - filenamePos) + ".engine";

    // Add the GPU device name to the file to ensure that the model is only used
    // on devices with the exact same GPU
    std::vector<std::string> deviceNames;
    getDeviceNames(deviceNames);

    if (static_cast<size_t>(options.deviceIndex) >= deviceNames.size()) {
        throw std::runtime_error("Error, provided device index is out of range!");
    }

    auto deviceName = deviceNames[options.deviceIndex];
    // Remove spaces from the device name
    deviceName.erase(std::remove_if(deviceName.begin(), deviceName.end(), ::isspace), deviceName.end());

    engineName += "." + deviceName;

    // Serialize the specified options into the filename
    if (options.precision == Precision::FP16) {
        engineName += ".fp16";
    } else if (options.precision == Precision::FP32) {
        engineName += ".fp32";
    } else {
        engineName += ".int8";
    }

    engineName += "." + std::to_string(options.maxBatchSize);
    engineName += "." + std::to_string(options.optBatchSize);

    return engineName;
}

template <typename T> void Engine<T>::getDeviceNames(std::vector<std::string> &deviceNames) {
    int numGPUs;
    cudaGetDeviceCount(&numGPUs);

    for (int device = 0; device < numGPUs; device++) {
        cudaDeviceProp prop;
        cudaGetDeviceProperties(&prop, device);

        deviceNames.push_back(std::string(prop.name));
    }
}

template <typename T>
cv::cuda::GpuMat Engine<T>::resizeKeepAspectRatioPadRightBottom(const cv::cuda::GpuMat &input, size_t height, size_t width,
                                                                const cv::Scalar &bgcolor) {
    float r = std::min(width / (input.cols * 1.0), height / (input.rows * 1.0));
    int unpad_w = r * input.cols;
    int unpad_h = r * input.rows;
    cv::cuda::GpuMat re(unpad_h, unpad_w, CV_8UC3);
    cv::cuda::resize(input, re, re.size());
    cv::cuda::GpuMat out(height, width, CV_8UC3, bgcolor);
    re.copyTo(out(cv::Rect(0, 0, re.cols, re.rows)));
    return out;
}

template <typename T>
void Engine<T>::transformOutput(std::vector<std::vector<std::vector<T>>> &input, std::vector<std::vector<T>> &output) {
    if (input.size() != 1) {
        throw std::logic_error("The feature vector has incorrect dimensions!");
    }

    output = std::move(input[0]);
}

template <typename T> void Engine<T>::transformOutput(std::vector<std::vector<std::vector<T>>> &input, std::vector<T> &output) {
    if (input.size() != 1 || input[0].size() != 1) {
        throw std::logic_error("The feature vector has incorrect dimensions!");
    }

    output = std::move(input[0][0]);
}

================
File: GDIOverlay.cpp
================
// GDIOverlay.cpp
#include "GDIOverlay.h"

GDIOverlay::GDIOverlay(HWND targetWnd, int screenWidth, int screenHeight) : targetWnd(targetWnd), _width(screenWidth), _height(screenHeight) {
    createOverlayWindow();
}

void GDIOverlay::createOverlayWindow() {
    overlayWnd = CreateWindowEx(WS_EX_LAYERED | WS_EX_TRANSPARENT | WS_EX_TOPMOST, "STATIC", NULL, WS_POPUP | WS_VISIBLE, 0, 0, _width,
                                _height, NULL, NULL, NULL, NULL);
    SetLayeredWindowAttributes(overlayWnd, RGB(0, 0, 0), 255, LWA_ALPHA);
    MARGINS margin = {-1};
    DwmExtendFrameIntoClientArea(overlayWnd, &margin);

    GetWindowRect(targetWnd, &rect);

    hdc = GetDC(overlayWnd);
    memDC = CreateCompatibleDC(hdc);
    hbmMem = CreateCompatibleBitmap(hdc, _width, _height);
    SelectObject(memDC, hbmMem);
    hbrBkgnd = CreateSolidBrush(RGB(0, 0, 0));

    hFont = CreateFont(20, 0, 0, 0, FW_NORMAL, FALSE, FALSE, FALSE, ANSI_CHARSET, OUT_DEFAULT_PRECIS, CLIP_DEFAULT_PRECIS,
                       DEFAULT_QUALITY, DEFAULT_PITCH | FF_SWISS, "Arial");

    // Create a pen for drawing
    hPen = CreatePen(PS_SOLID, 2, RGB(0, 0, 0));

    ShowWindow(overlayWnd, SW_SHOW);
}

void GDIOverlay::cleanScreen() {
    // Clear the memory DC
    FillRect(memDC, &rect, hbrBkgnd);
}

void GDIOverlay::drawDetectionsPen(const std::vector<Object> &detections) {
    // Calculate the offset to center the 640x640 capture zone
    int offsetX = (_width - 640) / 2;
    int offsetY = (_height - 640) / 2;

    // Draw the detections
    for (const auto &detection : detections) {
        int x = static_cast<int>(detection.rect.x) + offsetX;
        int y = static_cast<int>(detection.rect.y) + offsetY;
        int width = static_cast<int>(detection.rect.width);
        int height = static_cast<int>(detection.rect.height);

        // Change the pen color based on detection label
        SelectObject(memDC, PEN_LIST[detection.label % PEN_LIST.size()]);
        SelectObject(memDC, GetStockObject(NULL_BRUSH));

        // Draw the rectangle
        POINT points[5] = {
            {x, y}, {x + width, y}, {x + width, y + height}, {x, y + height}, {x, y} // Closing the rectangle
        };

        Polygon(memDC, points, 5);
    }
}

void GDIOverlay::drawDetections(const std::vector<Object> &detections) {
    // Calculate the offset to center the 640x640 capture zone
    int offsetX = (_width - 640) / 2;
    int offsetY = (_height - 640) / 2;

    // Draw the detections
    for (const auto &detection : detections) {
        auto brush = BRUSH_LIST[detection.label % BRUSH_LIST.size()];
        int x = static_cast<int>(detection.rect.x) + offsetX;
        int y = static_cast<int>(detection.rect.y) + offsetY;
        int width = static_cast<int>(detection.rect.width);
        int height = static_cast<int>(detection.rect.height);

        if (!brush) {
            std::cerr << "Failed to create brush." << std::endl;
            continue;
        }

        RECT detectionRect = {x, y, x + width, y + height};
        if (!FrameRect(memDC, &detectionRect, brush)) {
            std::cerr << "Failed to frame rect." << std::endl;
        }
    }
}

void GDIOverlay::drawText(const std::string &text, int x, int y, COLORREF color, int fontSize) {
    // Set the text color and background mode
    SetTextColor(memDC, color);
    SetBkMode(memDC, TRANSPARENT);

    HFONT hOldFont = (HFONT)SelectObject(memDC, hFont);

    // Draw the text line by line
    int lineHeight = fontSize + 2; // Adjust line height as needed
    int yPos = y;
    std::istringstream iss(text);
    std::string line;
    while (std::getline(iss, line)) {
        TextOut(memDC, x, yPos, line.c_str(), line.length());
        yPos += lineHeight;
    }
}

// Usage in your drawing or rendering function
void GDIOverlay::drawLog(const std::string &logMessage) {
    // Draw the log message
    drawText(logMessage, 10, 10, RGB(255, 0, 255), 20); // Adjust position and color as needed
}

void GDIOverlay::render() {
    // Copy the memory DC to the window DC
    BitBlt(hdc, 0, 0, _width, _height, memDC, 0, 0, SRCCOPY);
}

================
File: GDIOverlay.h
================
// GDIOverlay.h
#pragma once
#include "yolov8.h"
#include <vector>
#include <windows.h>
#include <dwmapi.h>

class GDIOverlay {
public:
    GDIOverlay(HWND targetWnd, int screenWidth, int screenHeight);
    void drawDetections(const std::vector<Object> &detections);
    void drawDetectionsPen(const std::vector<Object> &detections);
    void drawLog(const std::string &logMessage);
    void render();
    void cleanScreen();

private:
    HWND targetWnd;
    HWND overlayWnd;
    HDC hdc;
    HDC memDC;
    HBITMAP hbmMem;
    HBRUSH hbrBkgnd;
    HFONT hFont; 
    HPEN hPen;
    RECT rect;
    int _width;
    int _height;
    void createOverlayWindow();
    void drawText(const std::string &text, int x, int y, COLORREF color, int fontSize);

    const std::vector<std::tuple<int, int, int>> COLOR_LIST = {
        {255, 0, 0},   // Red
        {255, 120, 120}, // Light Red
        {0, 0, 255},   // Blue
        {120, 120, 255}  // Light Blue
    };

    const std::vector<HBRUSH> BRUSH_LIST = {
		CreateSolidBrush(RGB(255, 0, 0)),   // Red
		CreateSolidBrush(RGB(255, 150, 150)), // Light Red
		CreateSolidBrush(RGB(0, 0, 255)),   // Blue
		CreateSolidBrush(RGB(150, 150, 255))  // Light Blue
	};

    const std::vector<HPEN> PEN_LIST = {
        CreatePen(PS_SOLID, 2, RGB(255, 0, 0)),     // Red
        CreatePen(PS_DOT, 2, RGB(255, 150, 150)), // Light Red
        CreatePen(PS_SOLID, 2, RGB(0, 0, 255)),     // Blue
        CreatePen(PS_DOT, 2, RGB(150, 150, 255))  // Light Blue
    };
};

================
File: MouseController.cpp
================
#include "MouseController.h"


MouseController::MouseController(int screenWidth, int screenHeight, int detectionZoneWidth, int detectionZoneHeight, float sensitivity,
                                 int centralSquareSize, float minGain, float maxGain, float maxSpeed, int HL1, int HL2, int cpi)
    : screenWidth(screenWidth), screenHeight(screenHeight), detectionZoneWidth(detectionZoneWidth),
      detectionZoneHeight(detectionZoneHeight), sensitivity(sensitivity), centralSquareSize(centralSquareSize),
      minGain(minGain), maxSpeed(maxSpeed), maxGain(maxGain), hidDevice(nullptr), headLabel1(HL1), headLabel2(HL2), cpi(cpi) {
    // Calculate the top-left corner of the detection zone
    detectionZoneX = (screenWidth - detectionZoneWidth) / 2;
    detectionZoneY = (screenHeight - detectionZoneHeight) / 2;

    crosshairX = screenWidth / 2;
    crosshairY = screenHeight / 2;

    integralX = 0.0f;
    integralY = 0.0f;
    alpha = 0.35f;
    prevErrorX= 0.0f;
    prevErrorY = 0.0f;
    smoothedTargetX = 0.0f;
    smoothedTargetY = 0.0f;

    ConnectToDevice();
}

MouseController::~MouseController() {
    if (hidDevice) {
        CloseHandle(hidDevice);
    }
}

bool MouseController::ConnectToDevice() {
    // Initialize HID library
    GUID hidGuid;
    HidD_GetHidGuid(&hidGuid);

    // Get a handle to the device information set
    HDEVINFO deviceInfoSet = SetupDiGetClassDevs(&hidGuid, NULL, NULL, DIGCF_PRESENT | DIGCF_DEVICEINTERFACE);
    if (deviceInfoSet == INVALID_HANDLE_VALUE) {
        std::cerr << "Failed to get device information set" << std::endl;
        return false;
    }

    // Enumerate devices
    SP_DEVICE_INTERFACE_DATA deviceInterfaceData;
    deviceInterfaceData.cbSize = sizeof(SP_DEVICE_INTERFACE_DATA);
    DWORD deviceIndex = 0;

    while (SetupDiEnumDeviceInterfaces(deviceInfoSet, NULL, &hidGuid, deviceIndex++, &deviceInterfaceData)) {
        // Get the size of the device interface detail data
        DWORD requiredSize = 0;
        SetupDiGetDeviceInterfaceDetail(deviceInfoSet, &deviceInterfaceData, NULL, 0, &requiredSize, NULL);
        std::vector<BYTE> buffer(requiredSize);
        SP_DEVICE_INTERFACE_DETAIL_DATA *deviceInterfaceDetailData = reinterpret_cast<SP_DEVICE_INTERFACE_DETAIL_DATA *>(buffer.data());
        deviceInterfaceDetailData->cbSize = sizeof(SP_DEVICE_INTERFACE_DETAIL_DATA);

        // Get the device interface detail data
        if (SetupDiGetDeviceInterfaceDetail(deviceInfoSet, &deviceInterfaceData, deviceInterfaceDetailData, requiredSize, NULL, NULL)) {
            // Open a handle to the device
            HANDLE deviceHandle = CreateFile(deviceInterfaceDetailData->DevicePath, GENERIC_WRITE | GENERIC_READ,
                                             FILE_SHARE_READ | FILE_SHARE_WRITE, NULL, OPEN_EXISTING, 0, NULL);
            if (deviceHandle != INVALID_HANDLE_VALUE) {
                std::cout << "Device opened successfully: " << deviceInterfaceDetailData->DevicePath << std::endl;

                // Get the device attributes
                HIDD_ATTRIBUTES attributes;
                attributes.Size = sizeof(HIDD_ATTRIBUTES);
                if (HidD_GetAttributes(deviceHandle, &attributes)) {
                    if (attributes.VendorID == VENDOR_ID && attributes.ProductID == PRODUCT_ID) {
                        std::cout << "Found HIDVendor device" << std::endl;

                        // Get the serial number
                        wchar_t serialNumber[256];
                        if (HidD_GetSerialNumberString(deviceHandle, serialNumber, sizeof(serialNumber))) {
                            if (wcscmp(serialNumber, TARGET_SERIAL) == 0) {
                                // Check interface descriptor for vendor-defined usage page and usage
                                PHIDP_PREPARSED_DATA preparsedData;
                                HIDP_CAPS caps;
                                if (HidD_GetPreparsedData(deviceHandle, &preparsedData)) {
                                    if (HidP_GetCaps(preparsedData, &caps) == HIDP_STATUS_SUCCESS) {
                                        if (caps.UsagePage == TARGET_USAGE_PAGE && caps.Usage == TARGET_USAGE) {
                                            hidDevice = deviceHandle;
                                            HidD_FreePreparsedData(preparsedData);
                                            std::wcout << L"Found HIDVendor device with serial number: " << serialNumber << std::endl;
                                            std::cout << "Vendor ID: " << std::hex << attributes.VendorID << std::endl;
                                            std::cout << "Product ID: " << std::hex << attributes.ProductID << std::dec << std::endl;
                                            SetupDiDestroyDeviceInfoList(deviceInfoSet);
                                            return true; // Successfully connected to the device
                                        }
                                    }
                                    HidD_FreePreparsedData(preparsedData);
                                } else {
                                    std::cerr << "Failed to get preparsed data" << std::endl;
                                }
                            } else {
                                std::cerr << "Serial number does not match" << std::endl;
                            }
                        } else {
                            std::cerr << "Failed to get serial number" << std::endl;
                        }
                    }
                } else {
                    std::cerr << "Failed to get device attributes" << std::endl;
                }
                CloseHandle(deviceHandle);
            }
        }
    }

    SetupDiDestroyDeviceInfoList(deviceInfoSet);

    if (!hidDevice) {
        std::cerr << "No suitable HIDVendor device found." << std::endl;
        return false;
    }

    std::cout << "Device opened successfully" << std::endl;
    return true;
}

void MouseController::setCrosshairPosition(int x, int y) {
    crosshairX = x;
    crosshairY = y;
}

// Implementation of resetSpeed method
void MouseController::resetSpeed() {
    currentSpeedX = 0.0f;
    currentSpeedY = 0.0f;
    integralX = 0.0f;
    integralY = 0.0f;
    prevErrorX = 0.0f;
    prevErrorY = 0.0f;
    _dx = 0;
    _dy = 0;
}

void MouseController::processHIDReports() {
    while (running) {
        auto report = reportQueue.pop();
        processHIDReport(report);
    }
}

bool MouseController::processHIDReport(std::vector<uint8_t> &report) {
    if (hidDevice != nullptr) {
        BOOL res = HidD_SetOutputReport(hidDevice, report.data(), report.size());
        if (!res) {
            int err = GetLastError();
            std::cerr << "Failed to send HID report using HidD_SetOutputReport: " << err << std::endl;
            if (err == 995 || err == 1167) {
                std::cerr << "Device disconnected" << std::endl;
                CloseHandle(hidDevice);
                hidDevice = nullptr;
                std::this_thread::sleep_for(std::chrono::seconds(2)); // Wait for device to reconnect
                ConnectToDevice();
                std::cout << "\033[2J\033[H";
            } else {
                exit(1);
            }
        }
    } else {
        ConnectToDevice();
    }
    return true;
}

void MouseController::sendHIDReport(int16_t dx, int16_t dy) {
    std::vector<uint8_t> report(5, 0);

    report[0] = 0x06; // Report ID
    report[1] = dx & 0xFF;
    report[2] = (dx >> 8) & 0xFF;
    report[3] = dy & 0xFF;
    report[4] = (dy >> 8) & 0xFF;

   processHIDReport(report);
}

float MouseController::calculateSpeedScaling(const cv::Rect &rect) {
    // Define the thresholds for small, medium, and large detection boxes
    const float smallBoxThreshold = 7.0f;
    const float largeBoxThreshold = 40.0f;
    const float minScaling = 0.5f; // Minimum scaling factor
    const float maxScaling = 1.0f; // Maximum scaling factor

    float boxSize = (rect.width < rect.height) ? rect.width : rect.height; // Use the smaller dimension as the box size

    if (boxSize < smallBoxThreshold) {
        return minScaling; // Scale down to minScaling
    } else if (boxSize > largeBoxThreshold) {
        return maxScaling; // Use 100% of max speed
    } else {
        // Linearly interpolate between smallBoxThreshold and largeBoxThreshold
        return minScaling + (maxScaling - minScaling) * ((boxSize - smallBoxThreshold) / (largeBoxThreshold - smallBoxThreshold));
    }
}

void MouseController::aim(const std::vector<Object> &detections) {
    const float deadZoneThreshold = 5.0f;

    if (isLeftMouseButtonPressed() || isMouseButton5Pressed()) {
        Object closestDetection = findClosestDetection(detections);
        if (closestDetection.probability > 0.0f) {
            int targetX = closestDetection.rect.x + closestDetection.rect.width / 2;
            int targetY = closestDetection.rect.y + closestDetection.rect.height / 2;

            // Convert target coordinates from detection zone to screen coordinates
            int screenTargetX = detectionZoneX + targetX;
            int screenTargetY = detectionZoneY + targetY;

            // Convert crosshair coordinates from detection zone to screen coordinates
            int screenCrosshairX = detectionZoneX + crosshairX;
            int screenCrosshairY = detectionZoneY + crosshairY;

            // Calculate the relative movement
            int dx = static_cast<int>(screenTargetX - screenCrosshairX);
            int dy = static_cast<int>(screenTargetY - screenCrosshairY);

            // Calculate the distance to the target
            float distance = sqrt(dx * dx + dy * dy);

            // Ignore detections outside the central square or within the dead zone
            if (distance < deadZoneThreshold || dx < -centralSquareSize || dx > centralSquareSize || dy < -centralSquareSize ||
                dy > centralSquareSize) {
                resetSpeed();
                return;
            }

            // Calculate the maximum distance within the detection zone (diagonal of the detection zone)
            float detectionZoneDiagonal = sqrt(centralSquareSize * centralSquareSize + centralSquareSize * centralSquareSize);

            // Apply exponential scaling to the proportional gain
            float normalizedDistance = distance / detectionZoneDiagonal;
            float proportionalGain = minGain + (maxGain - minGain) * std::exp(-normalizedDistance);

            // Apply dynamic proportional control
            currentSpeedX = proportionalGain * dx;
            currentSpeedY = proportionalGain * dy;

            // Calculate the speed scaling factor based on the detection box size
            float speedScaling = calculateSpeedScaling(closestDetection.rect);

            // Scale the speed
            currentSpeedX *= speedScaling;
            currentSpeedY *= speedScaling;

            // Cap the speed to maxSpeed
            currentSpeedX = std::clamp(currentSpeedX, -maxSpeed, maxSpeed);
            currentSpeedY = std::clamp(currentSpeedY, -maxSpeed, maxSpeed);

            // Apply sensitivity and movement
            _dx = static_cast<int>(currentSpeedX * sensitivity);
            _dy = static_cast<int>(currentSpeedY * sensitivity);
        }
    } else {
        resetSpeed();
    }

    if (_dx != 0 || _dy != 0) {
        sendHIDReport(_dx, _dy);
    }
}




bool MouseController::isLeftMouseButtonPressed() { return (GetAsyncKeyState(VK_LBUTTON) & 0x8000) != 0; }
bool MouseController::isMouseButton4Pressed() { return (GetAsyncKeyState(VK_XBUTTON1) & 0x8000) != 0; } // Mouse Button 4
bool MouseController::isMouseButton5Pressed() { return (GetAsyncKeyState(VK_XBUTTON2) & 0x8000) != 0; } // Mouse Button 5

void MouseController::moveMouseRelative(int dx, int dy) {
    INPUT input = {0};
    input.type = INPUT_MOUSE;
    input.mi.dwFlags = MOUSEEVENTF_MOVE;
    input.mi.dx = dx;
    input.mi.dy = dy;
    SendInput(1, &input, sizeof(INPUT));
}

void MouseController::moveMouseAbsolute(int x, int y) {
    INPUT input = {0};
    input.type = INPUT_MOUSE;
    input.mi.dwFlags = MOUSEEVENTF_ABSOLUTE | MOUSEEVENTF_MOVE;
    input.mi.dx = static_cast<LONG>((x * 65535) / screenWidth);
    input.mi.dy = static_cast<LONG>((y * 65535) / screenHeight);
    SendInput(1, &input, sizeof(INPUT));
}

void MouseController::leftClick() {
    INPUT input = {0};
    input.type = INPUT_MOUSE;
    input.mi.dwFlags = MOUSEEVENTF_LEFTDOWN;
    SendInput(1, &input, sizeof(INPUT));

    ZeroMemory(&input, sizeof(INPUT));
    input.type = INPUT_MOUSE;
    input.mi.dwFlags = MOUSEEVENTF_LEFTUP;
    SendInput(1, &input, sizeof(INPUT));
}

// Modify findClosestDetection to use crosshair position
Object MouseController::findClosestDetection(const std::vector<Object> &detections) {
    Object closestDetection;
    closestDetection.probability = 0.0f; // Initialize with no detection

    float closestDistance = FLT_MAX;

    for (const auto &detection : detections) {
        if (detection.label == headLabel1 || detection.label == headLabel2) { // Replace with actual label values
            int detectionCenterX = detection.rect.x + detection.rect.width / 2;
            int detectionCenterY = detection.rect.y + detection.rect.height / 2;
            float distance = std::sqrt(std::pow(detectionCenterX - crosshairX, 2) + std::pow(detectionCenterY - crosshairY, 2));
            if (distance < closestDistance) {
                closestDistance = distance;
                closestDetection = detection;
            }
        }
    }

    return closestDetection;
}

void MouseController::triggerLeftClickIfCenterWithinDetection(const std::vector<Object> &detections) {
    if (isMouseButton4Pressed()) {
        int centerX = crosshairX;
        int centerY = crosshairY;

        for (const auto &detection : detections) {
            if (detection.label == 1 || detection.label == 3) { // Replace with actual label values
                if (centerX >= detection.rect.x && centerX <= detection.rect.x + detection.rect.width && centerY >= detection.rect.y &&
                    centerY <= detection.rect.y + detection.rect.height) {
                    leftClick();
                    Sleep(250);
                    break;
                }
            }
        }
    }
}

================
File: MouseController.h
================
#include "yolov8.h"
#include "threadsafe_queue.h"
#include <algorithm>
#include <cmath>
#include <iostream>
#include <vector>
#include <windows.h>
#include <setupapi.h>
#include <hidsdi.h>
#include <hidclass.h>

#define VENDOR_ID 0x0680
#define PRODUCT_ID 0x1b39
#define TARGET_SERIAL L"744DBD976574" // Replace with your target serial number
#define TARGET_USAGE_PAGE 0xFF00      // Vendor-defined usage page
#define TARGET_USAGE 0x01             // Vendor-defined usage

class MouseController {
public:
    MouseController(int screenWidth, int screenHeight, int detectionZoneWidth, int detectionZoneHeight, float sensitivity,
                    int centralSquareSize, float minGain, float maxGain, float maxSpeed, int HL1, int HL2, int cpi);
    ~MouseController();
    void aim(const std::vector<Object> &detections);
    void triggerLeftClickIfCenterWithinDetection(const std::vector<Object> &detections);
    void setCrosshairPosition(int x, int y);
    int getCrosshairX() const { return crosshairX; }
    int getCrosshairY() const { return crosshairY; }
    int getMovementX() const { return _dx; }
    int getMovementY() const { return _dy; }

private:
    int screenWidth;
    int screenHeight;
    int detectionZoneWidth;
    int detectionZoneHeight;
    int detectionZoneX;
    int detectionZoneY;
    int crosshairX;
    int crosshairY;
    int _dx;
    int _dy;
    float sensitivity;
    int centralSquareSize;
    int maxSpeed;
    float minGain;
    float maxGain;
    int currentSpeedX;
    int currentSpeedY;
    HANDLE hidDevice;
    float alpha; // Smoothing factor (between 0 and 1)
    float smoothedTargetX;
    float smoothedTargetY;

    // PID Controller state
    float integralX;
    float integralY;
    float prevErrorX;
    float prevErrorY;

    DWORD lastTime;

    int headLabel1;
    int headLabel2;

    int cpi;

    SafeQueue<std::vector<uint8_t>> reportQueue;
    std::thread reportThread;
    std::atomic<bool> running;

    bool isLeftMouseButtonPressed();
    bool isMouseButton4Pressed();
    bool isMouseButton5Pressed();
    void moveMouseRelative(int dx, int dy);
    void moveMouseAbsolute(int x, int y);
    void leftClick();
    Object findClosestDetection(const std::vector<Object> &detections);
    void sendHIDReport(int16_t dx, int16_t dy);
    bool ConnectToDevice();
    bool processHIDReport(std::vector<uint8_t> &report);
    void processHIDReports();
    void resetSpeed();
    float calculateSpeedScaling(const cv::Rect &rect);
};

================
File: object_detection_image_mt.cpp
================
#define NOMINMAX

#include <opencv2/cudaarithm.hpp>
#include <opencv2/cudafilters.hpp>
#include <opencv2/cudawarping.hpp>
#include <opencv2/opencv.hpp>
#include <opencv2/cudaimgproc.hpp>
#include <opencv2/core/cuda.hpp>

#include "DXGICapture.h"
#include "GDIOverlay.h"
#include "MouseController.h"
#include "threadsafe_queue.h"
#include "yolov8.h"

#include <algorithm>
#include <atomic>
#include <chrono>
#include <stdlib.h>
#include <thread>
#include <variant>
#include <windows.h>
#include <future>

using namespace std::chrono;
using YoloV8Variant = std::variant<YoloV8<float>, YoloV8<__half>>;

LRESULT CALLBACK WindowProc(HWND hwnd, UINT uMsg, WPARAM wParam, LPARAM lParam) {
    if (uMsg == WM_DESTROY) {
        PostQuitMessage(0);
        return 0;
    }
    return DefWindowProc(hwnd, uMsg, wParam, lParam);
}

// Function to detect crosshair using OpenCV template matching and draw around it
cv::Point detectCrosshairCPU(const cv::Mat &frame, const cv::Mat &templ) {
    // Perform template matching
    cv::Mat result;
    cv::matchTemplate(frame, templ, result, cv::TM_CCOEFF_NORMED);

    // Localize the best match with minMaxLoc
    double minVal, maxVal;
    cv::Point minLoc, maxLoc;
    cv::minMaxLoc(result, &minVal, &maxVal, &minLoc, &maxLoc);

    cv::Point crosshairCenter = cv::Point(maxLoc.x + templ.cols / 2, maxLoc.y + templ.rows / 2);

    // Draw a rectangle around the detected crosshair in the original colored frame
    cv::rectangle(frame, maxLoc, cv::Point(maxLoc.x + templ.cols, maxLoc.y + templ.rows), cv::Scalar(0, 255, 0), 2);

    return crosshairCenter;
}

cv::Point detectCrosshairGPU(const cv::cuda::GpuMat &frame, const cv::cuda::GpuMat &templ) {
    cv::cuda::GpuMat result;
    cv::Size resultSize(frame.cols - templ.cols + 1, frame.rows - templ.rows + 1);
    result.create(resultSize, CV_32FC1);

    // Create TemplateMatching object
    cv::Ptr<cv::cuda::TemplateMatching> matcher = cv::cuda::createTemplateMatching(frame.type(), cv::TemplateMatchModes::TM_CCOEFF_NORMED);

    // Perform template matching on GPU
    matcher->match(frame, templ, result);

    // Download the result to CPU memory
    cv::Mat resultCpu;
    result.download(resultCpu);

    // Find the best match location
    double minVal, maxVal;
    cv::Point minLoc, maxLoc;
    cv::minMaxLoc(resultCpu, &minVal, &maxVal, &minLoc, &maxLoc);

    // Calculate the center of the crosshair
    cv::Point crosshairCenter(maxLoc.x + templ.cols / 2, maxLoc.y + templ.rows / 2);

    return crosshairCenter;
}

// Function to move the cursor to the top of the log section
void moveToTop() {
    std::cout << "\033[H"; // Move the cursor to the top-left corner
}

void clearScreen() {
    std::cout << "\033[2J\033[H"; // Clear the screen and move the cursor to the top-left corner
}

void logAverageLatencies(LatencyQueue &captureLatency, LatencyQueue &detectionLatency, LatencyQueue &overlayLatency,
                         SafeQueue<std::string> &logQueue) {
    // Clear the screen and move the cursor to the top
    moveToTop();

    // Get the average latencies
    double avgCaptureLatency = captureLatency.getAverageLatency();
    double avgDetectionLatency = detectionLatency.getAverageLatency();
    double avgOverlayLatency = overlayLatency.getAverageLatency();

    std::cout << "Average Capture Latency (last 500ms): " << std::to_string(avgCaptureLatency) + "ms\n"
              << "Average Detection Latency (last 500ms): " << std::to_string(avgDetectionLatency) + "ms\n"
              << "Average Overlay Latency (last 500ms): " << std::to_string(avgOverlayLatency) + "ms\n";
}

class Runners {
public:
    void captureThread(FrameQueue &captureQueue, LatencyQueue &latencyQueue, DXGICapture &capture, HWND targetWnd, int captureWidth,
                       int captureHeight, std::atomic<bool> &running) {
        const int targetFPS = 1000;
        const std::chrono::nanoseconds frameDuration(1000000000 / targetFPS); // 1s / FPS
        cv::Mat frame1(captureHeight, captureWidth, CV_8UC3);
        cv::Mat frame2(captureHeight, captureWidth, CV_8UC3);
        cv::Mat *currentFrame = &frame1;
        cv::Mat *nextFrame = &frame2;

        while (running) {
            auto start = std::chrono::high_resolution_clock::now();

            capture.CaptureScreen(*currentFrame);

            if (!currentFrame->empty()) {
                captureQueue.push(std::move(*currentFrame)); // Use move semantics to avoid copying
            }

            std::swap(currentFrame, nextFrame);

            auto end = std::chrono::high_resolution_clock::now();
            auto duration = end - start;

            latencyQueue.push(std::chrono::duration_cast<std::chrono::milliseconds>(duration).count());

        }
    }

void detectionThread(FrameQueue &captureQueue, DetectionQueue &detectionQueue, LatencyQueue &latencyQueue, YoloV8Variant &yoloV8,
                         MouseController &mouseController, cv::Mat &templateImg, int captureWidth, int captureHeight,
                         std::atomic<bool> &running) {
        const int targetFPS = 1000;
        const std::chrono::nanoseconds frameDuration(1000000000 / targetFPS);

        while (running) {
            cv::Mat frame = captureQueue.pop();

            auto detectionStartTime = std::chrono::high_resolution_clock::now();

            // Calculate the region of interest (center capture area)
            int centerX = frame.cols / 2;
            int centerY = frame.rows / 2;

            // Define the capture ROI within the frame
            int roiWidth = captureWidth;
            int roiHeight = captureHeight;
            int x = std::max(centerX - roiWidth / 2, 0);
            int y = std::max(centerY - roiHeight / 2, 0);
            if (x + roiWidth > frame.cols)
                x = frame.cols - roiWidth;
            if (y + roiHeight > frame.rows)
                y = frame.rows - roiHeight;
            cv::Rect roi(x, y, roiWidth, roiHeight);

            // Define the smaller 160x160 ROI within the capture frame
            int smallRoiSize = 160;
            int smallX = std::max(centerX - smallRoiSize / 2, 0);
            int smallY = std::max(centerY - smallRoiSize / 2, 0);
            if (smallX + smallRoiSize > frame.cols)
                smallX = frame.cols - smallRoiSize;
            if (smallY + smallRoiSize > frame.rows)
                smallY = frame.rows - smallRoiSize;
            cv::Rect smallRoi(smallX, smallY, smallRoiSize, smallRoiSize);

            // Use CPU matrices for the ROIs
            cv::Mat croppedFrame = frame(roi);
            cv::Mat smallCroppedFrame = frame(smallRoi);

            cv::Point crosshairPos = detectCrosshairCPU(smallCroppedFrame, templateImg);

            // Adjust the coordinates to the full frame
            crosshairPos.x += smallX - x;
            crosshairPos.y += smallY - y;

            // Perform object detection on the captured frame
            std::vector<Object> detections;
            std::visit([&](auto &yolo) { detections = yolo.detectObjects(croppedFrame); }, yoloV8);

            // Use the crosshairPos for further processing, like mouse control, etc.
            mouseController.setCrosshairPosition(crosshairPos.x, crosshairPos.y);
            mouseController.aim(detections);
            mouseController.triggerLeftClickIfCenterWithinDetection(detections);

            detectionQueue.push(detections);

            auto detectionEndTime = std::chrono::high_resolution_clock::now();
            auto detectionDuration = detectionEndTime - detectionStartTime;

            latencyQueue.push(std::chrono::duration_cast<std::chrono::milliseconds>(detectionDuration).count());
        }
    }

    void overlayThread(DetectionQueue &detectionQueue, LatencyQueue &latencyQueue, SafeQueue<std::string> &logQueue, GDIOverlay &overlay,
                       std::atomic<bool> &running) {
        const int targetFPS = 500;
        const std::chrono::nanoseconds frameDuration(1000000000 / targetFPS);

        MSG msg = {};
        while (msg.message != WM_QUIT || running) {
            auto overlayStartTime = std::chrono::high_resolution_clock::now();

            if (PeekMessage(&msg, nullptr, 0, 0, PM_REMOVE)) {
                TranslateMessage(&msg);
                DispatchMessage(&msg);
            } else {
                overlay.cleanScreen();
                //overlay.drawLog("Yup");
                overlay.drawDetectionsPen(detectionQueue.pop());
                overlay.render();
                auto overlayEndTime = std::chrono::high_resolution_clock::now();
                auto overlayDuration = overlayEndTime - overlayStartTime;

                if (overlayDuration < frameDuration) {
                    std::this_thread::sleep_for(frameDuration - overlayDuration);
                    // Busy wait for the remaining time
                    while (std::chrono::high_resolution_clock::now() - overlayEndTime < frameDuration - overlayDuration) {
                        // Busy wait
                    }
                }

                overlayEndTime = std::chrono::high_resolution_clock::now();
                overlayDuration = overlayEndTime - overlayStartTime;
                latencyQueue.push(std::chrono::duration_cast<std::chrono::milliseconds>(overlayDuration).count());
            }
        }
    }
};

int main(int argc, char *argv[]) {
    try {
        YoloV8Config config;
        std::string onnxModelPath, precision;
        int captureWidth, captureHeight, HL1, HL2;

        if (argc != 7) {
            std::cerr << "Usage: " << argv[0] << " <path to TensorRT engine file> <width> <height> <float/half> [Head Label ID 1][Head Label ID 2]" << std::endl;
            return -1;
        }

        onnxModelPath = argv[1];
        captureHeight = strtol(argv[2], NULL, 10);
        captureWidth = strtol(argv[3], NULL, 10);
        precision = argv[4];
        HL1 = strtol(argv[5], NULL, 10);
        HL2 = strtol(argv[6], NULL, 10);

        std::unique_ptr<YoloV8Variant> yoloV8;

        if (precision == "float") {
            config.precision = Precision::FP32;
            yoloV8 = std::make_unique<YoloV8Variant>(YoloV8<float>(onnxModelPath, config));
        } else if (precision == "half") {
            config.precision = Precision::FP16; 
            yoloV8 = std::make_unique<YoloV8Variant>(YoloV8<__half>(onnxModelPath, config));
        } else {
            std::cerr << "Invalid precision: " << precision << ". Use 'float' or 'half'." << std::endl;
            return -1;
        }

        // Get the screen dimensions
        int screenWidth = GetSystemMetrics(SM_CXSCREEN);
        int screenHeight = GetSystemMetrics(SM_CYSCREEN);

        // Create MouseController instance
        MouseController mouseController(screenWidth, screenHeight, captureWidth, captureHeight, 0.80f, 55, 0.25f, 0.65f, 15, HL1, HL2, 3000);

        cv::Mat templateImg = cv::imread("crosshair.png", cv::IMREAD_COLOR);
        if (templateImg.empty()) {
            std::cerr << "Failed to load template image" << std::endl;
            return -1;
        }

        SafeQueue<std::string> logQueue;
        LatencyQueue captureLatency;
        LatencyQueue detectionLatency;
        LatencyQueue overlayLatency;
        std::atomic<bool> running(true);
        FrameQueue captureQueue;
        DetectionQueue detectionQueue;

        HWND targetWnd = FindWindow(NULL, "Counter-Strike 2");
        if (!targetWnd) {
            std::cerr << "Failed to find target window." << std::endl;
            return -1;
        }

        DXGICapture capture(targetWnd);
        //GDIOverlay overlay(targetWnd, screenWidth, screenHeight);

        Runners runners;

        std::thread captureThreadObj(
            [&runners, &captureQueue, &captureLatency, &capture, targetWnd, captureWidth, captureHeight, &running]() {
                runners.captureThread(captureQueue, captureLatency, capture, targetWnd, captureWidth, captureHeight, running);
            });

        std::thread detectionThreadObj([&runners, &captureQueue, &detectionQueue, &detectionLatency, &yoloV8, &mouseController,
                                        &templateImg, captureWidth, captureHeight, &running]() {
            runners.detectionThread(captureQueue, detectionQueue, detectionLatency, *yoloV8, mouseController, templateImg, captureWidth,
                                    captureHeight, running);
        });

        //std::thread overlayThreadObj([&runners, &detectionQueue, &overlayLatency, &logQueue, &overlay, &running]() {
		//	runners.overlayThread(detectionQueue, overlayLatency, logQueue, overlay, running);
		//});

        clearScreen();
        while (true) {
            std::string logMessage;
            MSG msg;

            if (PeekMessage(&msg, NULL, 0, 0, PM_REMOVE)) {
                if (msg.message == WM_QUIT) {
                    running = false;
                }
                TranslateMessage(&msg);
                DispatchMessage(&msg);
            }

            // Check for INS key press
            if (GetAsyncKeyState(VK_INSERT) & 0x8000) {
                running = false;
                break;
            }

            logAverageLatencies(captureLatency, detectionLatency, overlayLatency, logQueue);
        }

        running = false;
        logQueue.push("exit");
        cv::destroyAllWindows();
        if (captureThreadObj.joinable())
            captureThreadObj.join();
        if (detectionThreadObj.joinable())
            detectionThreadObj.join();
        //if (overlayThreadObj.joinable())
        //    overlayThreadObj.join();
    } catch (const std::exception &e) {
        std::cerr << "Error: " << e.what() << std::endl;
        return -1;
    }

    return 0;
}

================
File: shader.hlsl
================
cbuffer ConstantBuffer : register(b0) { matrix WorldViewProjection; }

struct VS_INPUT {
    float3 Pos : POSITION;
    float2 Tex : TEXCOORD;
};

struct PS_INPUT {
    float4 Pos : SV_POSITION;
    float2 Tex : TEXCOORD;
};

PS_INPUT VS(VS_INPUT input) {
    PS_INPUT output;
    output.Pos = float4(input.Pos, 1.0f);
    output.Tex = input.Tex;
    return output;
}

Texture2D tex : register(t0);
SamplerState samLinear : register(s0);

float4 PS(PS_INPUT input) : SV_Target { return tex.Sample(samLinear, input.Tex); }

================
File: threadsafe_queue.h
================
#pragma once

#include <chrono>
#include <condition_variable>
#include <mutex>
#include <opencv2/opencv.hpp>
#include <thread>

template <typename T> class SafeQueue {
private:
    std::queue<T> queue;
    mutable std::mutex mutex;
    std::condition_variable cv;

public:
    // Push using perfect forwarding
    template <typename U> void push(U &&value) {
        std::lock_guard<std::mutex> lock(mutex);
        queue.push(std::forward<U>(value));
        cv.notify_one();
    }

    T pop() {
        std::unique_lock<std::mutex> lock(mutex);
        cv.wait(lock, [this] { return !queue.empty(); });
        T value = std::move(queue.front());
        queue.pop();
        return value;
    }

    bool empty() const {
        std::lock_guard<std::mutex> lock(mutex);
        return queue.empty();
    }
};

class DetectionQueue {
public:
    void push(const std::vector<Object> &detections) {
        std::lock_guard<std::mutex> lock(mutex_);
        if (hasMovedMoreThanThreshold(detections, 5)) {
            detections_ = detections;
        }
    }

    std::vector<Object> pop() {
        std::lock_guard<std::mutex> lock(mutex_);
        return detections_;
    }

private:
    bool hasMovedMoreThanThreshold(const std::vector<Object> &newDetections, int threshold) {
        if (newDetections.size() != detections_.size()) {
            return true; // Different number of detections, consider it moved
        }
        for (size_t i = 0; i < newDetections.size(); ++i) {
            int dx = newDetections[i].rect.x - detections_[i].rect.x;
            int dy = newDetections[i].rect.y - detections_[i].rect.y;
            if (std::abs(dx) > threshold || std::abs(dy) > threshold) {
                return true;
            }
        }
        return false;
    }

    std::vector<Object> detections_;
    std::mutex mutex_;
};

class LatencyQueue {
public:
    void push(long long latency) {
        std::lock_guard<std::mutex> lock(mutex_);
        auto now = std::chrono::steady_clock::now();
        queue_.push_back({latency, now});

        // Remove old entries
        while (!queue_.empty() && std::chrono::duration_cast<std::chrono::milliseconds>(now - queue_.front().timestamp).count() > 500) {
            queue_.pop_front();
        }
    }

    double getAverageLatency() {
        std::lock_guard<std::mutex> lock(mutex_);
        if (queue_.empty())
            return 0.0;

        double totalLatency = 0.0;
        int count = 0;
        for (const auto &entry : queue_) {
            totalLatency += entry.latency;
            ++count;
        }
        return totalLatency / count;
    }

private:
    struct LatencyEntry {
        long long latency;
        std::chrono::steady_clock::time_point timestamp;
    };

    std::deque<LatencyEntry> queue_;
    std::mutex mutex_;
};

class FrameQueue {
public:
    void push(const cv::Mat &frame) {
        std::lock_guard<std::mutex> lock(mutex_);
        if (!queue_.empty()) {
            queue_.pop(); // Remove the old frame to keep only the latest one
        }
        queue_.push(frame); // Directly push without cloning
        cond_var_.notify_one();
    }

    cv::Mat pop() {
        std::unique_lock<std::mutex> lock(mutex_);
        cond_var_.wait(lock, [this] { return !queue_.empty(); });
        cv::Mat frame = queue_.front();
        queue_.pop();
        return frame;
    }

private:
    std::queue<cv::Mat> queue_;
    std::mutex mutex_;
    std::condition_variable cond_var_;
};

================
File: yolov8.cpp
================
#include "yolov8.h"
#include <opencv2/cudaimgproc.hpp>

template <typename T> YoloV8<T>::YoloV8(const std::string &onnxModelPath, const YoloV8Config &config)
    : PROBABILITY_THRESHOLD(config.probabilityThreshold), NMS_THRESHOLD(config.nmsThreshold), TOP_K(config.topK),
      SEG_CHANNELS(config.segChannels), SEG_H(config.segH), SEG_W(config.segW), SEGMENTATION_THRESHOLD(config.segmentationThreshold),
      CLASS_NAMES(config.classNames), NUM_KPS(config.numKPS), KPS_THRESHOLD(config.kpsThreshold) {
    // Specify options for GPU inference
    Options options;
    options.optBatchSize = 1;
    options.maxBatchSize = 1;

    options.precision = config.precision;
    options.calibrationDataDirectoryPath = config.calibrationDataDirectory;

    if (options.precision == Precision::INT8) {
        if (options.calibrationDataDirectoryPath.empty()) {
            throw std::runtime_error("Error: Must supply calibration data path for INT8 calibration");
        }
    }

    // Create our TensorRT inference engine
    m_trtEngine = std::make_unique<Engine<T>>(options);
    std::cout << "Building or loading TensorRT engine..." << std::endl;

    // Build the onnx model into a TensorRT engine file, cache the file to disk, and then load the TensorRT engine file into memory.
    // If the engine file already exists on disk, this function will not rebuild but only load into memory.
    // The engine file is rebuilt any time the above Options are changed.
    auto succ = m_trtEngine->buildLoadNetwork(onnxModelPath, SUB_VALS, DIV_VALS, NORMALIZE);
    if (!succ) {
        const std::string errMsg = "Error: Unable to build or load the TensorRT engine. "
                                   "Try increasing TensorRT log severity to kVERBOSE (in /libs/tensorrt-cpp-api/engine.cpp).";
        throw std::runtime_error(errMsg);
    }
    std::cout << "TensorRT engine built and loaded!" << std::endl;
}

template <typename T>  std::vector<std::vector<cv::cuda::GpuMat>> YoloV8<T>::preprocess(const cv::cuda::GpuMat &gpuImg) {
    // Populate the input vectors
    const auto &inputDims = m_trtEngine->getInputDims();

    // Convert the image from BGR to RGB
    cv::cuda::GpuMat rgbMat;
    cv::cuda::cvtColor(gpuImg, rgbMat, cv::COLOR_BGR2RGB);

    auto resized = rgbMat;

    // Resize to the model expected input size while maintaining the aspect ratio with the use of padding
    if (resized.rows != inputDims[0].d[1] || resized.cols != inputDims[0].d[2]) {
        // Only resize if not already the right size to avoid unecessary copy
        resized = Engine<float>::resizeKeepAspectRatioPadRightBottom(rgbMat, inputDims[0].d[1], inputDims[0].d[2]);
    }

    // Convert to format expected by our inference engine
    // The reason for the strange format is because it supports models with multiple inputs as well as batching
    // In our case though, the model only has a single input and we are using a batch size of 1.
    std::vector<cv::cuda::GpuMat> input{std::move(resized)};
    std::vector<std::vector<cv::cuda::GpuMat>> inputs{std::move(input)};

    // These params will be used in the post-processing stage
    m_imgHeight = rgbMat.rows;
    m_imgWidth = rgbMat.cols;
    m_ratio = 1.f / std::min(inputDims[0].d[2] / static_cast<float>(rgbMat.cols), inputDims[0].d[1] / static_cast<float>(rgbMat.rows));

    return inputs;
}

template <typename T> std::vector<Object> YoloV8<T>::detectObjects(const cv::cuda::GpuMat &inputImageBGR) {
    // Preprocess the input image
    const auto input = preprocess(inputImageBGR);

    // Run inference using the TensorRT engine
    std::vector<std::vector<std::vector<T>>> featureVectors;
    auto succ = m_trtEngine->runInference(input, featureVectors);
    if (!succ) {
        throw std::runtime_error("Error: Unable to run inference.");
    }
    // Check if our model does only object detection or also supports segmentation
    std::vector<Object> ret;
    const auto &numOutputs = m_trtEngine->getOutputDims().size();
    if (numOutputs == 1) {
        // Object detection or pose estimation
        // Since we have a batch size of 1 and only 1 output, we must convert the output from a 3D array to a 1D array.
        std::vector<T> featureVector;
        Engine<T>::transformOutput(featureVectors, featureVector);

        const auto &outputDims = m_trtEngine->getOutputDims();
        int numChannels = outputDims[outputDims.size() - 1].d[1];
        // TODO: Need to improve this to make it more generic (don't use magic number).
        // For now it works with Ultralytics pretrained models.
        if (numChannels == 56) {
            // Pose estimation
            ret = postprocessPose(featureVector);
        } else {
            // Object detection
            ret = postprocessDetect(featureVector);
        }
    } else {
        // Segmentation
        // Since we have a batch size of 1 and 2 outputs, we must convert the output from a 3D array to a 2D array.
        std::vector<std::vector<T>> featureVector;
        Engine<T>::transformOutput(featureVectors, featureVector);
        ret = postProcessSegmentation(featureVector);
    }
    return ret;
}

template <typename T> std::vector<Object> YoloV8<T>::detectObjects(const cv::Mat &inputImageBGR) {
    // Upload the image to GPU memory
    cv::cuda::GpuMat gpuImg;
    gpuImg.upload(inputImageBGR);

    // Call detectObjects with the GPU image
    return detectObjects(gpuImg);
}

template <typename T> std::vector<Object> YoloV8<T>::postProcessSegmentation(std::vector<std::vector<T>> &featureVectors) {
    const auto &outputDims = m_trtEngine->getOutputDims();

    int numChannels = outputDims[0].d[1];
    int numAnchors = outputDims[0].d[2];

    const auto numClasses = numChannels - SEG_CHANNELS - 4;

    // Ensure the output lengths are correct
    if (featureVectors[0].size() != static_cast<size_t>(numChannels) * numAnchors) {
        throw std::logic_error("Output at index 0 has incorrect length");
    }

    if (featureVectors[1].size() != static_cast<size_t>(SEG_CHANNELS) * SEG_H * SEG_W) {
        throw std::logic_error("Output at index 1 has incorrect length");
    }

    cv::Mat output = cv::Mat(numChannels, numAnchors, CV_32F, featureVectors[0].data());
    output = output.t();

    cv::Mat protos = cv::Mat(SEG_CHANNELS, SEG_H * SEG_W, CV_32F, featureVectors[1].data());

    std::vector<int> labels;
    std::vector<float> scores;
    std::vector<cv::Rect> bboxes;
    std::vector<cv::Mat> maskConfs;
    std::vector<int> indices;

    // Object the bounding boxes and class labels
    for (int i = 0; i < numAnchors; i++) {
        auto rowPtr = output.row(i).ptr<float>();
        auto bboxesPtr = rowPtr;
        auto scoresPtr = rowPtr + 4;
        auto maskConfsPtr = rowPtr + 4 + numClasses;
        auto maxSPtr = std::max_element(scoresPtr, scoresPtr + numClasses);
        float score = *maxSPtr;
        if (score > PROBABILITY_THRESHOLD) {
            float x = *bboxesPtr++;
            float y = *bboxesPtr++;
            float w = *bboxesPtr++;
            float h = *bboxesPtr;

            float x0 = std::clamp((x - 0.5f * w) * m_ratio, 0.f, m_imgWidth);
            float y0 = std::clamp((y - 0.5f * h) * m_ratio, 0.f, m_imgHeight);
            float x1 = std::clamp((x + 0.5f * w) * m_ratio, 0.f, m_imgWidth);
            float y1 = std::clamp((y + 0.5f * h) * m_ratio, 0.f, m_imgHeight);

            int label = maxSPtr - scoresPtr;
            cv::Rect_<float> bbox;
            bbox.x = x0;
            bbox.y = y0;
            bbox.width = x1 - x0;
            bbox.height = y1 - y0;

            cv::Mat maskConf = cv::Mat(1, SEG_CHANNELS, CV_32F, maskConfsPtr);

            bboxes.push_back(bbox);
            labels.push_back(label);
            scores.push_back(score);
            maskConfs.push_back(maskConf);
        }
    }

    // Require OpenCV 4.7 for this function
    cv::dnn::NMSBoxesBatched(bboxes, scores, labels, PROBABILITY_THRESHOLD, NMS_THRESHOLD, indices);

    // Obtain the segmentation masks
    cv::Mat masks;
    std::vector<Object> objs;
    int cnt = 0;
    for (auto &i : indices) {
        if (cnt >= TOP_K) {
            break;
        }
        cv::Rect tmp = bboxes[i];
        Object obj;
        obj.label = labels[i];
        obj.rect = tmp;
        obj.probability = scores[i];
        masks.push_back(maskConfs[i]);
        objs.push_back(obj);
        cnt += 1;
    }

    // Convert segmentation mask to original frame
    if (!masks.empty()) {
        cv::Mat matmulRes = (masks * protos).t();
        cv::Mat maskMat = matmulRes.reshape(indices.size(), {SEG_W, SEG_H});

        std::vector<cv::Mat> maskChannels;
        cv::split(maskMat, maskChannels);
        const auto inputDims = m_trtEngine->getInputDims();

        cv::Rect roi;
        if (m_imgHeight > m_imgWidth) {
            roi = cv::Rect(0, 0, SEG_W * m_imgWidth / m_imgHeight, SEG_H);
        } else {
            roi = cv::Rect(0, 0, SEG_W, SEG_H * m_imgHeight / m_imgWidth);
        }

        for (size_t i = 0; i < indices.size(); i++) {
            cv::Mat dest, mask;
            cv::exp(-maskChannels[i], dest);
            dest = 1.0 / (1.0 + dest);
            dest = dest(roi);
            cv::resize(dest, mask, cv::Size(static_cast<int>(m_imgWidth), static_cast<int>(m_imgHeight)), cv::INTER_LINEAR);
            objs[i].boxMask = mask(objs[i].rect) > SEGMENTATION_THRESHOLD;
        }
    }

    return objs;
}

template <typename T> std::vector<Object> YoloV8<T>::postprocessPose(std::vector<T> &featureVector) {
    const auto &outputDims = m_trtEngine->getOutputDims();
    auto numChannels = outputDims[0].d[1];
    auto numAnchors = outputDims[0].d[2];

    std::vector<cv::Rect> bboxes;
    std::vector<float> scores;
    std::vector<int> labels;
    std::vector<int> indices;
    std::vector<std::vector<float>> kpss;

    cv::Mat output = cv::Mat(numChannels, numAnchors, CV_32F, featureVector.data());
    output = output.t();

    // Get all the YOLO proposals
    for (int i = 0; i < numAnchors; i++) {
        auto rowPtr = output.row(i).ptr<float>();
        auto bboxesPtr = rowPtr;
        auto scoresPtr = rowPtr + 4;
        auto kps_ptr = rowPtr + 5;
        float score = *scoresPtr;
        if (score > PROBABILITY_THRESHOLD) {
            float x = *bboxesPtr++;
            float y = *bboxesPtr++;
            float w = *bboxesPtr++;
            float h = *bboxesPtr;

            float x0 = std::clamp((x - 0.5f * w) * m_ratio, 0.f, m_imgWidth);
            float y0 = std::clamp((y - 0.5f * h) * m_ratio, 0.f, m_imgHeight);
            float x1 = std::clamp((x + 0.5f * w) * m_ratio, 0.f, m_imgWidth);
            float y1 = std::clamp((y + 0.5f * h) * m_ratio, 0.f, m_imgHeight);

            cv::Rect_<float> bbox;
            bbox.x = x0;
            bbox.y = y0;
            bbox.width = x1 - x0;
            bbox.height = y1 - y0;

            std::vector<float> kps;
            for (int k = 0; k < NUM_KPS; k++) {
                float kpsX = *(kps_ptr + 3 * k) * m_ratio;
                float kpsY = *(kps_ptr + 3 * k + 1) * m_ratio;
                float kpsS = *(kps_ptr + 3 * k + 2);
                kpsX = std::clamp(kpsX, 0.f, m_imgWidth);
                kpsY = std::clamp(kpsY, 0.f, m_imgHeight);
                kps.push_back(kpsX);
                kps.push_back(kpsY);
                kps.push_back(kpsS);
            }

            bboxes.push_back(bbox);
            labels.push_back(0); // All detected objects are people
            scores.push_back(score);
            kpss.push_back(kps);
        }
    }

    // Run NMS
    cv::dnn::NMSBoxesBatched(bboxes, scores, labels, PROBABILITY_THRESHOLD, NMS_THRESHOLD, indices);

    std::vector<Object> objects;

    // Choose the top k detections
    int cnt = 0;
    for (auto &chosenIdx : indices) {
        if (cnt >= TOP_K) {
            break;
        }

        Object obj{};
        obj.probability = scores[chosenIdx];
        obj.label = labels[chosenIdx];
        obj.rect = bboxes[chosenIdx];
        obj.kps = kpss[chosenIdx];
        objects.push_back(obj);

        cnt += 1;
    }

    return objects;
}

template <typename T> std::vector<Object> YoloV8<T>::postprocessDetect(std::vector<T> &featureVector) {
    const auto &outputDims = m_trtEngine->getOutputDims();
    auto numChannels = outputDims[0].d[1];
    auto numAnchors = outputDims[0].d[2];

    auto numClasses = CLASS_NAMES.size();

    std::vector<cv::Rect> bboxes;
    std::vector<float> scores;
    std::vector<int> labels;
    std::vector<int> indices;

    cv::Mat output = cv::Mat(numChannels, numAnchors, CV_32F, featureVector.data());
    output = output.t();

    // Get all the YOLO proposals
    for (int i = 0; i < numAnchors; i++) {
        auto rowPtr = output.row(i).ptr<float>();
        auto bboxesPtr = rowPtr;
        auto scoresPtr = rowPtr + 4;
        auto maxSPtr = std::max_element(scoresPtr, scoresPtr + numClasses);
        float score = *maxSPtr;
        if (score > PROBABILITY_THRESHOLD) {
            float x = *bboxesPtr++;
            float y = *bboxesPtr++;
            float w = *bboxesPtr++;
            float h = *bboxesPtr;

            float x0 = std::clamp((x - 0.5f * w) * m_ratio, 0.f, m_imgWidth);
            float y0 = std::clamp((y - 0.5f * h) * m_ratio, 0.f, m_imgHeight);
            float x1 = std::clamp((x + 0.5f * w) * m_ratio, 0.f, m_imgWidth);
            float y1 = std::clamp((y + 0.5f * h) * m_ratio, 0.f, m_imgHeight);

            int label = maxSPtr - scoresPtr;
            cv::Rect_<float> bbox;
            bbox.x = x0;
            bbox.y = y0;
            bbox.width = x1 - x0;
            bbox.height = y1 - y0;

            bboxes.push_back(bbox);
            labels.push_back(label);
            scores.push_back(score);
        }
    }

    // Run NMS
    cv::dnn::NMSBoxesBatched(bboxes, scores, labels, PROBABILITY_THRESHOLD, NMS_THRESHOLD, indices);

    std::vector<Object> objects;

    // Choose the top k detections
    int cnt = 0;
    for (auto &chosenIdx : indices) {
        if (cnt >= TOP_K) {
            break;
        }

        Object obj{};
        obj.probability = scores[chosenIdx];
        obj.label = labels[chosenIdx];
        obj.rect = bboxes[chosenIdx];
        objects.push_back(obj);

        cnt += 1;
    }

    return objects;
}

template <typename T> void YoloV8<T>::drawObjectLabels(cv::Mat &image, const std::vector<Object> &objects, unsigned int scale, int squareHalfSize) {
    // Get the center of the screen
    int screenCenterX = image.cols / 2;
    int screenCenterY = image.rows / 2;

    // If segmentation information is present, start with that
    if (!objects.empty() && !objects[0].boxMask.empty()) {
        cv::Mat mask = image.clone();
        for (const auto &object : objects) {
            // Choose the color
            int colorIndex = object.label % CS2_COLORS.size(); // We have only defined 80 unique colors
            cv::Scalar color = cv::Scalar(CS2_COLORS[colorIndex][0], CS2_COLORS[colorIndex][1], CS2_COLORS[colorIndex][2]);

            // Add the mask for said object
            mask(object.rect).setTo(color * 255, object.boxMask);
        }
        // Add all the masks to our image
        cv::addWeighted(image, 0.5, mask, 0.8, 1, image);
    }

    // Bounding boxes and annotations
    for (auto &object : objects) {
        // Choose the color
        int colorIndex = object.label % CS2_COLORS.size(); // We have only defined 80 unique colors
        cv::Scalar color = cv::Scalar(CS2_COLORS[colorIndex][0], CS2_COLORS[colorIndex][1], CS2_COLORS[colorIndex][2]);

        const auto &rect = object.rect;

        // Draw rectangles
        cv::rectangle(image, rect, color * 255, scale + 1);

        // Pose estimation
        if (!object.kps.empty()) {
            auto &kps = object.kps;
            for (int k = 0; k < NUM_KPS + 2; k++) {
                if (k < NUM_KPS) {
                    int kpsX = std::round(kps[k * 3]);
                    int kpsY = std::round(kps[k * 3 + 1]);
                    float kpsS = kps[k * 3 + 2];
                    if (kpsS > KPS_THRESHOLD) {
                        cv::Scalar kpsColor = cv::Scalar(KPS_COLORS[k][0], KPS_COLORS[k][1], KPS_COLORS[k][2]);
                        cv::circle(image, {kpsX, kpsY}, 5, kpsColor, -1);
                    }
                }
                auto &ske = SKELETON[k];
                int pos1X = std::round(kps[(ske[0] - 1) * 3]);
                int pos1Y = std::round(kps[(ske[0] - 1) * 3 + 1]);

                int pos2X = std::round(kps[(ske[1] - 1) * 3]);
                int pos2Y = std::round(kps[(ske[1] - 1) * 3 + 1]);

                float pos1S = kps[(ske[0] - 1) * 3 + 2];
                float pos2S = kps[(ske[1] - 1) * 3 + 2];

                if (pos1S > KPS_THRESHOLD && pos2S > KPS_THRESHOLD) {
                    cv::Scalar limbColor = cv::Scalar(LIMB_COLORS[k][0], LIMB_COLORS[k][1], LIMB_COLORS[k][2]);
                    cv::line(image, {pos1X, pos1Y}, {pos2X, pos2Y}, limbColor, 2);
                }
            }
        }
    }
}

// Explicit instantiation of the template
template class YoloV8<float>;
template class YoloV8<__half>;

================
File: yolov8.h
================
#pragma once
#include "engine.h"
#include <fstream>
#include <algorithm>

// Utility method for checking if a file exists on disk
inline bool doesFileExist(const std::string &name) {
    std::ifstream f(name.c_str());
    return f.good();
}

struct Object {
    // The object class.
    int label{};
    // The detection's confidence probability.
    float probability{};
    // The object bounding box rectangle.
    cv::Rect_<float> rect;
    // Semantic segmentation mask
    cv::Mat boxMask;
    // Pose estimation key points
    std::vector<float> kps{};
};

// Config the behavior of the YoloV8 detector.
// Can pass these arguments as command line parameters.
struct YoloV8Config {
    // The precision to be used for inference
    Precision precision = Precision::FP16;
    // Calibration data directory. Must be specified when using INT8 precision.
    std::string calibrationDataDirectory;
    // Probability threshold used to filter detected objects
    float probabilityThreshold = 0.25f;
    // Non-maximum suppression threshold
    float nmsThreshold = 0.65f;
    // Max number of detected objects to return
    int topK = 100;
    // Segmentation config options
    int segChannels = 32;
    int segH = 160;
    int segW = 160;
    float segmentationThreshold = 0.5f;
    // Pose estimation options
    int numKPS = 17;
    float kpsThreshold = 0.5f;
    // Class thresholds (default are COCO classes)
    std::vector<std::string> classNames = {"c", "ch", "t", "th"};
};

template <typename T> class YoloV8 {
public:
    // Builds the onnx model into a TensorRT engine, and loads the engine into memory
    YoloV8(const std::string &onnxModelPath, const YoloV8Config &config);

    // Detect the objects in the image
    std::vector<Object> detectObjects(const cv::Mat &inputImageBGR);
    std::vector<Object> detectObjects(const cv::cuda::GpuMat &inputImageBGR);

    // Draw the object bounding boxes and labels on the image
    void drawObjectLabels(cv::Mat &image, const std::vector<Object> &objects, unsigned int scale = 2, int squareHalfSize = 0);
    // Object classes as strings
    const std::vector<std::string> CLASS_NAMES;

private:
    // Preprocess the input
    std::vector<std::vector<cv::cuda::GpuMat>> preprocess(const cv::cuda::GpuMat &gpuImg);

    // Postprocess the output
    std::vector<Object> postprocessDetect(std::vector<T> &featureVector);

    // Postprocess the output for pose model
    std::vector<Object> postprocessPose(std::vector<T> &featureVector);

    // Postprocess the output for segmentation model
    std::vector<Object> postProcessSegmentation(std::vector<std::vector<T>> &featureVectors);

    std::unique_ptr<Engine<T>> m_trtEngine = nullptr;

    // Used for image preprocessing
    // YoloV8 model expects values between [0.f, 1.f] so we use the following params
    const std::array<float, 3> SUB_VALS{0.f, 0.f, 0.f};
    const std::array<float, 3> DIV_VALS{1.f, 1.f, 1.f};
    const bool NORMALIZE = true;

    float m_ratio = 1;
    float m_imgWidth = 0;
    float m_imgHeight = 0;

    // Filter thresholds
    const float PROBABILITY_THRESHOLD;
    const float NMS_THRESHOLD;
    const int TOP_K;

    // Segmentation constants
    const int SEG_CHANNELS;
    const int SEG_H;
    const int SEG_W;
    const float SEGMENTATION_THRESHOLD;

    // Pose estimation constant
    const int NUM_KPS;
    const float KPS_THRESHOLD;

    const std::vector<std::vector<float>> CS2_COLORS = {
        {1.0, 0.0, 0.0}, // Red
        {1.0, 0.5, 0.5}, // Light Red
		{0.0, 0.0, 1.0}, // Blue
		{0.5, 0.5, 1.0}  // Light Blue
    };

    // Color list for drawing objects
    const std::vector<std::vector<float>> COLOR_LIST = {{1, 1, 1},
                                                        {0.098, 0.325, 0.850},
                                                        {0.125, 0.694, 0.929},
                                                        {0.556, 0.184, 0.494},
                                                        {0.188, 0.674, 0.466},
                                                        {0.933, 0.745, 0.301},
                                                        {0.184, 0.078, 0.635},
                                                        {0.300, 0.300, 0.300},
                                                        {0.600, 0.600, 0.600},
                                                        {0.000, 0.000, 1.000},
                                                        {0.000, 0.500, 1.000},
                                                        {0.000, 0.749, 0.749},
                                                        {0.000, 1.000, 0.000},
                                                        {1.000, 0.000, 0.000},
                                                        {1.000, 0.000, 0.667},
                                                        {0.000, 0.333, 0.333},
                                                        {0.000, 0.667, 0.333},
                                                        {0.000, 1.000, 0.333},
                                                        {0.000, 0.333, 0.667},
                                                        {0.000, 0.667, 0.667},
                                                        {0.000, 1.000, 0.667},
                                                        {0.000, 0.333, 1.000},
                                                        {0.000, 0.667, 1.000},
                                                        {0.000, 1.000, 1.000},
                                                        {0.500, 0.333, 0.000},
                                                        {0.500, 0.667, 0.000},
                                                        {0.500, 1.000, 0.000},
                                                        {0.500, 0.000, 0.333},
                                                        {0.500, 0.333, 0.333},
                                                        {0.500, 0.667, 0.333},
                                                        {0.500, 1.000, 0.333},
                                                        {0.500, 0.000, 0.667},
                                                        {0.500, 0.333, 0.667},
                                                        {0.500, 0.667, 0.667},
                                                        {0.500, 1.000, 0.667},
                                                        {0.500, 0.000, 1.000},
                                                        {0.500, 0.333, 1.000},
                                                        {0.500, 0.667, 1.000},
                                                        {0.500, 1.000, 1.000},
                                                        {1.000, 0.333, 0.000},
                                                        {1.000, 0.667, 0.000},
                                                        {1.000, 1.000, 0.000},
                                                        {1.000, 0.000, 0.333},
                                                        {1.000, 0.333, 0.333},
                                                        {1.000, 0.667, 0.333},
                                                        {1.000, 1.000, 0.333},
                                                        {1.000, 0.000, 0.667},
                                                        {1.000, 0.333, 0.667},
                                                        {1.000, 0.667, 0.667},
                                                        {1.000, 1.000, 0.667},
                                                        {1.000, 0.000, 1.000},
                                                        {1.000, 0.333, 1.000},
                                                        {1.000, 0.667, 1.000},
                                                        {0.000, 0.000, 0.333},
                                                        {0.000, 0.000, 0.500},
                                                        {0.000, 0.000, 0.667},
                                                        {0.000, 0.000, 0.833},
                                                        {0.000, 0.000, 1.000},
                                                        {0.000, 0.167, 0.000},
                                                        {0.000, 0.333, 0.000},
                                                        {0.000, 0.500, 0.000},
                                                        {0.000, 0.667, 0.000},
                                                        {0.000, 0.833, 0.000},
                                                        {0.000, 1.000, 0.000},
                                                        {0.167, 0.000, 0.000},
                                                        {0.333, 0.000, 0.000},
                                                        {0.500, 0.000, 0.000},
                                                        {0.667, 0.000, 0.000},
                                                        {0.833, 0.000, 0.000},
                                                        {1.000, 0.000, 0.000},
                                                        {0.000, 0.000, 0.000},
                                                        {0.143, 0.143, 0.143},
                                                        {0.286, 0.286, 0.286},
                                                        {0.429, 0.429, 0.429},
                                                        {0.571, 0.571, 0.571},
                                                        {0.714, 0.714, 0.714},
                                                        {0.857, 0.857, 0.857},
                                                        {0.741, 0.447, 0.000},
                                                        {0.741, 0.717, 0.314},
                                                        {0.000, 0.500, 0.500}};

    const std::vector<std::vector<unsigned int>> KPS_COLORS = {
        {0, 255, 0},    {0, 255, 0},    {0, 255, 0},    {0, 255, 0},    {0, 255, 0},   {255, 128, 0},
        {255, 128, 0},  {255, 128, 0},  {255, 128, 0},  {255, 128, 0},  {255, 128, 0}, {51, 153, 255},
        {51, 153, 255}, {51, 153, 255}, {51, 153, 255}, {51, 153, 255}, {51, 153, 255}};

    const std::vector<std::vector<unsigned int>> SKELETON = {{16, 14}, {14, 12}, {17, 15}, {15, 13}, {12, 13}, {6, 12}, {7, 13},
                                                             {6, 7},   {6, 8},   {7, 9},   {8, 10},  {9, 11},  {2, 3},  {1, 2},
                                                             {1, 3},   {2, 4},   {3, 5},   {4, 6},   {5, 7}};

    const std::vector<std::vector<unsigned int>> LIMB_COLORS = {
        {51, 153, 255}, {51, 153, 255}, {51, 153, 255}, {51, 153, 255}, {255, 51, 255}, {255, 51, 255}, {255, 51, 255},
        {255, 128, 0},  {255, 128, 0},  {255, 128, 0},  {255, 128, 0},  {255, 128, 0},  {0, 255, 0},    {0, 255, 0},
        {0, 255, 0},    {0, 255, 0},    {0, 255, 0},    {0, 255, 0},    {0, 255, 0}};
};
